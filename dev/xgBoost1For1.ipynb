{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple models using  MultiOutputClassifier\n",
    "\n",
    "1. xgboost is the base model\n",
    "2. CV with 5 samples/ not folds\n",
    "3. Importances from CV \n",
    "4. Var reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessX(df):\n",
    "    '''\n",
    "    Preprocessing for independent  vars\n",
    "    encode categoricals\n",
    "    \n",
    "    returns processed df,\n",
    "    '''\n",
    "    df['cp_dose'] = (df['cp_dose'] == 'D1').astype(int)\n",
    "    df['cp_type'] = (df['cp_type'] == 'trt_cp').astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xAll = pd.read_csv('../Data/train_features.csv')\n",
    "yAll = pd.read_csv('../Data/train_targets_scored.csv')\n",
    "\n",
    "xAll = PreProcessX(xAll)\n",
    "\n",
    "idList=['sig_id']\n",
    "# xAll=xAll.drop(idList,axis=1)\n",
    "# yAll=yAll.drop(idList,axis=1)\n",
    "\n",
    "trainIds= xAll['sig_id'].sample(frac=0.7,random_state=SEED)\n",
    "xTrain = xAll[xAll['sig_id'].isin(trainIds)]\n",
    "yTrain = yAll[yAll['sig_id'].isin(trainIds)]\n",
    "xValid = xAll[~xAll['sig_id'].isin(trainIds)]\n",
    "yValid = yAll[~yAll['sig_id'].isin(trainIds)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist',random_state=SEED))\n",
    "\n",
    "# clf = Pipeline([('classify', classifier)])\n",
    "\n",
    "# params = {'classify__estimator__colsample_bytree': 0.3,\n",
    "#           'classify__estimator__gamma': 1,\n",
    "#           'classify__estimator__learning_rate': 0.07,\n",
    "#           'classify__estimator__max_depth': 6,\n",
    "#           'classify__estimator__min_child_weight': 15,\n",
    "#           'classify__estimator__max_delta_step': 1,\n",
    "#           'classify__estimator__n_estimators': 800,\n",
    "#           'classify__estimator__grow_policy': 'lossguide',\n",
    "#           'classify__estimator__max_leaves': 24\n",
    "#          }\n",
    "\n",
    "# _ = clf.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 41s, sys: 9.33 s, total: 11min 50s\n",
      "Wall time: 11min 52s\n"
     ]
    }
   ],
   "source": [
    "#%time multiModels= clf.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumRows =xTrain.shape[0]\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    #'num_class':2,\n",
    "    'max_depth' : 10, # for gpu hist\n",
    "    'eta' : 0.05,\n",
    "    'colsample_bytree' : 0.2,\n",
    "    'min_child_weight' : int(NumRows*0.001),\n",
    "    'gamma': 1,\n",
    "    'tree_method':'gpu_hist',\n",
    "    'grow_policy': 'lossguide',\n",
    "    'max_leaves': 32, ## to constraint tree to be shallower\n",
    "    'n_estimators': 900,\n",
    "    'max_delta_step':1\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "xgMod = XGBClassifier(**params,random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### single model for setup\n",
    "Parameters were changed for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 42s, sys: 9.39 s, total: 12min 51s\n",
      "Wall time: 12min 52s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%time  classifier = MultiOutputClassifier(xgMod).fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.0108962807635367 valid loss0.01722624312747751\n"
     ]
    }
   ],
   "source": [
    "##diff model\n",
    "trainPreds = classifier.predict_proba(xTrain)\n",
    "trainPreds = np.array(trainPreds)[:,:,1].T\n",
    "trainLoss = log_loss(np.ravel(yTrain), np.ravel(trainPreds))\n",
    "\n",
    "validPreds = classifier.predict_proba(xValid)\n",
    "validPreds = np.array(validPreds)[:,:,1].T\n",
    "valLoss = log_loss(np.ravel(yValid), np.ravel(validPreds))\n",
    "print(f'train loss {trainLoss} valid loss{valLoss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importances from 5 models\n",
    "\n",
    "https://stats.stackexchange.com/questions/162162/relative-variable-importance-for-boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taran/depot/Projects/kaggleDev/venv/lib/python3.6/site-packages/xgboost/sklearn.py:695: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n",
      "/home/taran/depot/Projects/kaggleDev/venv/lib/python3.6/site-packages/xgboost/sklearn.py:695: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n",
      "/home/taran/depot/Projects/kaggleDev/venv/lib/python3.6/site-packages/xgboost/sklearn.py:695: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n",
      "/home/taran/depot/Projects/kaggleDev/venv/lib/python3.6/site-packages/xgboost/sklearn.py:695: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n",
      "/home/taran/depot/Projects/kaggleDev/venv/lib/python3.6/site-packages/xgboost/sklearn.py:695: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 6min 52s, sys: 1min 8s, total: 1h 8min 1s\n",
      "Wall time: 1h 4min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.016818835437180883,\n",
       " 0.017034743542869636,\n",
       " 0.016899042059087096,\n",
       " 0.016770016914106314,\n",
       " 0.016851989914719055]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "validLosses = []\n",
    "validPreds = []\n",
    "featImp = []\n",
    "for i in range(5):\n",
    "    \n",
    "    trainIds= xAll['sig_id'].sample(frac=0.7,random_state=(SEED +100*i + 10*i +i ))\n",
    "    xTrain = xAll[xAll['sig_id'].isin(trainIds)]\n",
    "    yTrain = yAll[yAll['sig_id'].isin(trainIds)]\n",
    "    xValid = xAll[~xAll['sig_id'].isin(trainIds)]\n",
    "    yValid = yAll[~yAll['sig_id'].isin(trainIds)]\n",
    " \n",
    "    xTrain=xTrain.drop(idList,axis=1)\n",
    "    xValid=xValid.drop(idList,axis=1)\n",
    "    yTrain=yTrain.drop(idList,axis=1)\n",
    "    yValid=yValid.drop(idList,axis=1)\n",
    "                                    \n",
    "                                    \n",
    "    #fit model\n",
    "    clf = MultiOutputClassifier(xgMod).fit(xTrain, yTrain)\n",
    "    #get loss\n",
    "    validPredsIn = clf.predict_proba(xValid)\n",
    "    validPredsIn = np.array(validPredsIn)[:,:,1].T\n",
    "    validPreds.append(validPredsIn)\n",
    "    valLoss = log_loss(np.ravel(yValid), np.ravel(validPredsIn))\n",
    "    validLosses.append(valLoss)\n",
    "    #get importances\n",
    "    InnerImp = [] \n",
    "    #multioutput importances\n",
    "    for model in clf.estimators_:\n",
    "        InnerImp.append(model.feature_importances_)\n",
    "    featImp.append(np.nanmean(InnerImp, axis=0) )   \n",
    "        \n",
    "validLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = InnerImp[0]/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    imp += InnerImp[i]/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Imp</th>\n",
       "      <th>CummProp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g-280</td>\n",
       "      <td>0.027026</td>\n",
       "      <td>0.027026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g-140</td>\n",
       "      <td>0.016387</td>\n",
       "      <td>0.043413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g-202</td>\n",
       "      <td>0.016236</td>\n",
       "      <td>0.059649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g-659</td>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.075667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c-63</td>\n",
       "      <td>0.015875</td>\n",
       "      <td>0.091542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>g-573</td>\n",
       "      <td>0.015630</td>\n",
       "      <td>0.107171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>g-386</td>\n",
       "      <td>0.015235</td>\n",
       "      <td>0.122406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>g-653</td>\n",
       "      <td>0.014255</td>\n",
       "      <td>0.136661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>g-61</td>\n",
       "      <td>0.014054</td>\n",
       "      <td>0.150715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c-98</td>\n",
       "      <td>0.013339</td>\n",
       "      <td>0.164054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       Imp  CummProp\n",
       "0  g-280  0.027026  0.027026\n",
       "1  g-140  0.016387  0.043413\n",
       "2  g-202  0.016236  0.059649\n",
       "3  g-659  0.016018  0.075667\n",
       "4   c-63  0.015875  0.091542\n",
       "5  g-573  0.015630  0.107171\n",
       "6  g-386  0.015235  0.122406\n",
       "7  g-653  0.014255  0.136661\n",
       "8   g-61  0.014054  0.150715\n",
       "9   c-98  0.013339  0.164054"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = pd.DataFrame({'Imp': imp}, index=xTrain.columns).sort_values(by='Imp', ascending=False)\n",
    "imp = imp.reset_index()\n",
    "\n",
    "imp['CummProp'] = imp['Imp'].cumsum()\n",
    "imp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.to_csv('../dumps/VarImp_xgboostAllCv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Imp</th>\n",
       "      <th>CummProp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g-280</td>\n",
       "      <td>0.027026</td>\n",
       "      <td>0.027026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g-140</td>\n",
       "      <td>0.016387</td>\n",
       "      <td>0.043413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g-202</td>\n",
       "      <td>0.016236</td>\n",
       "      <td>0.059649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g-659</td>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.075667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c-63</td>\n",
       "      <td>0.015875</td>\n",
       "      <td>0.091542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>g-273</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.897932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>g-295</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.898371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>g-366</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.898809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>g-760</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.899246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>g-357</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.899683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       Imp  CummProp\n",
       "0    g-280  0.027026  0.027026\n",
       "1    g-140  0.016387  0.043413\n",
       "2    g-202  0.016236  0.059649\n",
       "3    g-659  0.016018  0.075667\n",
       "4     c-63  0.015875  0.091542\n",
       "..     ...       ...       ...\n",
       "491  g-273  0.000440  0.897932\n",
       "492  g-295  0.000438  0.898371\n",
       "493  g-366  0.000438  0.898809\n",
       "494  g-760  0.000437  0.899246\n",
       "495  g-357  0.000437  0.899683\n",
       "\n",
       "[496 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp[imp['CummProp'] <= 0.90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "496 variables contribute 90\n",
    "\n",
    "Single model top 90% cummulative vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_90 = imp[imp['CummProp'] <= 0.90]['index'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0## will correspond to first model in the cv\n",
    "trainIds= xAll['sig_id'].sample(frac=0.7,random_state=(SEED +100*i + 10*i +i ))\n",
    "xTrain = xAll[xAll['sig_id'].isin(trainIds)]\n",
    "yTrain = yAll[yAll['sig_id'].isin(trainIds)]\n",
    "xValid = xAll[~xAll['sig_id'].isin(trainIds)]\n",
    "yValid = yAll[~yAll['sig_id'].isin(trainIds)]\n",
    " \n",
    "xTrain=xTrain.drop(idList,axis=1)\n",
    "xValid=xValid.drop(idList,axis=1)\n",
    "yTrain=yTrain.drop(idList,axis=1)\n",
    "yValid=yValid.drop(idList,axis=1)\n",
    "                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016931147285010367"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                    \n",
    "#fit model\n",
    "clf = MultiOutputClassifier(xgMod).fit(xTrain[vars_90], yTrain)\n",
    "    #get loss \n",
    "    \n",
    "validPredsIn = clf.predict_proba(xValid[vars_90])\n",
    "validPredsIn = np.array(validPredsIn)[:,:,1].T\n",
    "valLoss = log_loss(np.ravel(yValid), np.ravel(validPredsIn))\n",
    "valLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dropping no vars we have loss of 0.016818835437180883\n",
    "##### dropping 90 % vars we have loss of 0.016930555069928717 \n",
    "##### dropping 80 % vars we have loss of 0.016970555069928717 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_90 = vars_90[::-1]#### sorting in least important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping least important variables \n",
    "def dropVars(sVars,stLoss,end,st=1,step=1):\n",
    "    '''\n",
    "    sVars variable list starting from least important variables\n",
    "    \n",
    "    drop variables upto end increasingly from st to end\n",
    "    for large number of variables step size should be increased, \n",
    "    dropping step number of variables at a time\n",
    "    stLoss thw loss with which we started\n",
    "    returns result dropping variables\n",
    "    '''\n",
    "    results ={}\n",
    "    for i in range(st,end,step):\n",
    "        \n",
    "        remainingVars = sVars[i:] # drop variable\n",
    "        clf = MultiOutputClassifier(xgMod).fit(xTrain[remainingVars], yTrain)\n",
    "    #get loss\n",
    "    \n",
    "        trainPreds = clf.predict_proba(xTrain[remainingVars])\n",
    "        trainPreds = np.array(trainPreds)[:,:,1].T\n",
    "        trainLoss = log_loss(np.ravel(yTrain), np.ravel(trainPreds))\n",
    "    \n",
    "        validPredsIn = clf.predict_proba(xValid[remainingVars])\n",
    "        validPredsIn = np.array(validPredsIn)[:,:,1].T\n",
    "        testLoss = log_loss(np.ravel(yValid), np.ravel(validPredsIn))\n",
    "        \n",
    "        results[f'dropped_{i}'] = (trainLoss,testLoss,(stLoss-testLoss))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 19min 55s, sys: 1min 13s, total: 1h 21min 8s\n",
      "Wall time: 1h 6min 29s\n"
     ]
    }
   ],
   "source": [
    "### this means we are running upto 180th least important variable, will drop30 at a time\n",
    "%time dropVarList = dropVars(vars_90,stLoss=valLoss,end=300,st=30,step=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropped_30': (0.006886144792808197,\n",
       "  0.016909976390229228,\n",
       "  2.1170894781138982e-05),\n",
       " 'dropped_60': (0.006955824830327545,\n",
       "  0.0169299435187507,\n",
       "  1.2037662596650622e-06),\n",
       " 'dropped_90': (0.007039574014816243,\n",
       "  0.016913640929654316,\n",
       "  1.7506355356050607e-05),\n",
       " 'dropped_120': (0.007132542296729811,\n",
       "  0.016945910936591826,\n",
       "  -1.4763651581459897e-05),\n",
       " 'dropped_150': (0.00723659253425806,\n",
       "  0.016948801934310937,\n",
       "  -1.7654649300570535e-05),\n",
       " 'dropped_180': (0.007360061050018332,\n",
       "  0.016909988336489032,\n",
       "  2.115894852133468e-05),\n",
       " 'dropped_210': (0.007496262822378197,\n",
       "  0.016926166355366336,\n",
       "  4.980929644030074e-06),\n",
       " 'dropped_240': (0.007681756325197782,\n",
       "  0.016921182943515117,\n",
       "  9.96434149524994e-06),\n",
       " 'dropped_270': (0.007890067848747462,\n",
       "  0.016952872598220894,\n",
       "  -2.1725313210527175e-05)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropVarList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropResult = pd.DataFrame(dropVarList).T.rename_axis('VariableDrops').reset_index()\n",
    "dropResult.columns = ['VariableDrops','trainLoss','testLoss','DiffWithSt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VariableDrops</th>\n",
       "      <th>trainLoss</th>\n",
       "      <th>testLoss</th>\n",
       "      <th>DiffWithSt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dropped_30</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>0.016910</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dropped_60</td>\n",
       "      <td>0.006956</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dropped_90</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>0.016914</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dropped_120</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.016946</td>\n",
       "      <td>-0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dropped_150</td>\n",
       "      <td>0.007237</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>-0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dropped_180</td>\n",
       "      <td>0.007360</td>\n",
       "      <td>0.016910</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dropped_210</td>\n",
       "      <td>0.007496</td>\n",
       "      <td>0.016926</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dropped_240</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.016921</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dropped_270</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>-0.000022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VariableDrops  trainLoss  testLoss  DiffWithSt\n",
       "0    dropped_30   0.006886  0.016910    0.000021\n",
       "1    dropped_60   0.006956  0.016930    0.000001\n",
       "2    dropped_90   0.007040  0.016914    0.000018\n",
       "3   dropped_120   0.007133  0.016946   -0.000015\n",
       "4   dropped_150   0.007237  0.016949   -0.000018\n",
       "5   dropped_180   0.007360  0.016910    0.000021\n",
       "6   dropped_210   0.007496  0.016926    0.000005\n",
       "7   dropped_240   0.007682  0.016921    0.000010\n",
       "8   dropped_270   0.007890  0.016953   -0.000022"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x7f566c4da438>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = dropResult['DiffWithSt'].plot()\n",
    "ax.hlines(0, 0, 8, color='g',lw=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal decrease in Performance even after dropping 270 variables, out of 495, we can reduce more,\n",
    "This time reducing the step size\n",
    "\n",
    "Looking at train loss and test loss we can see that reducing number of variables leads to less overfitting due to decrease in complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropVarList2 = dropVars(vars_90,stLoss=valLoss,end=450,st=290,step=20)## 8 more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VariableDrops</th>\n",
       "      <th>trainLoss</th>\n",
       "      <th>testLoss</th>\n",
       "      <th>DiffWithSt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dropped_290</td>\n",
       "      <td>0.008051</td>\n",
       "      <td>0.016960</td>\n",
       "      <td>-0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dropped_310</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>0.017026</td>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dropped_330</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.017039</td>\n",
       "      <td>-0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dropped_350</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>0.017096</td>\n",
       "      <td>-0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dropped_370</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>0.017211</td>\n",
       "      <td>-0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dropped_390</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>0.017215</td>\n",
       "      <td>-0.000284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dropped_410</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>0.017237</td>\n",
       "      <td>-0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dropped_430</td>\n",
       "      <td>0.011056</td>\n",
       "      <td>0.017264</td>\n",
       "      <td>-0.000333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VariableDrops  trainLoss  testLoss  DiffWithSt\n",
       "0   dropped_290   0.008051  0.016960   -0.000029\n",
       "1   dropped_310   0.008259  0.017026   -0.000095\n",
       "2   dropped_330   0.008492  0.017039   -0.000108\n",
       "3   dropped_350   0.008775  0.017096   -0.000165\n",
       "4   dropped_370   0.009147  0.017211   -0.000280\n",
       "5   dropped_390   0.009619  0.017215   -0.000284\n",
       "6   dropped_410   0.010274  0.017237   -0.000306\n",
       "7   dropped_430   0.011056  0.017264   -0.000333"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropResult2 = pd.DataFrame(dropVarList2).T.rename_axis('VariableDrops').reset_index()\n",
    "dropResult2.columns = ['VariableDrops','trainLoss','testLoss','DiffWithSt']\n",
    "dropResult2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x7f56502c0128>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = dropResult2['DiffWithSt'].plot()\n",
    "ax.hlines(0, 0, 8, color='g',lw=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the clear drop, so from this step, we will drop up to 240 variables since there is no perf drop at this level,\n",
    "\n",
    "To further reduce variables we will use correlations, adn drop correlated variables, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrVars = vars_90[270:][::-1] #dropping 240 vars, and sorting importance wise, most to least\n",
    "\n",
    "len(corrVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g-280</th>\n",
       "      <th>g-140</th>\n",
       "      <th>g-202</th>\n",
       "      <th>g-659</th>\n",
       "      <th>c-63</th>\n",
       "      <th>g-573</th>\n",
       "      <th>g-386</th>\n",
       "      <th>g-653</th>\n",
       "      <th>g-61</th>\n",
       "      <th>c-98</th>\n",
       "      <th>...</th>\n",
       "      <th>g-609</th>\n",
       "      <th>g-186</th>\n",
       "      <th>g-587</th>\n",
       "      <th>g-629</th>\n",
       "      <th>g-702</th>\n",
       "      <th>g-87</th>\n",
       "      <th>c-54</th>\n",
       "      <th>g-400</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>g-280</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.404634</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>0.100349</td>\n",
       "      <td>0.246456</td>\n",
       "      <td>0.127427</td>\n",
       "      <td>0.236207</td>\n",
       "      <td>0.137636</td>\n",
       "      <td>0.084422</td>\n",
       "      <td>0.243840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017336</td>\n",
       "      <td>0.161116</td>\n",
       "      <td>0.276532</td>\n",
       "      <td>0.245769</td>\n",
       "      <td>0.086189</td>\n",
       "      <td>0.222741</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.024834</td>\n",
       "      <td>0.296697</td>\n",
       "      <td>0.288032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-140</th>\n",
       "      <td>0.404634</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>0.454765</td>\n",
       "      <td>0.616351</td>\n",
       "      <td>0.142014</td>\n",
       "      <td>0.661661</td>\n",
       "      <td>0.261368</td>\n",
       "      <td>0.427394</td>\n",
       "      <td>0.591029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130393</td>\n",
       "      <td>0.528228</td>\n",
       "      <td>0.272609</td>\n",
       "      <td>0.645036</td>\n",
       "      <td>0.431011</td>\n",
       "      <td>0.300633</td>\n",
       "      <td>0.601226</td>\n",
       "      <td>0.405114</td>\n",
       "      <td>0.625284</td>\n",
       "      <td>0.611929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-202</th>\n",
       "      <td>0.004110</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.097759</td>\n",
       "      <td>0.042053</td>\n",
       "      <td>0.031437</td>\n",
       "      <td>0.121548</td>\n",
       "      <td>0.049936</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.163773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>0.029867</td>\n",
       "      <td>0.017984</td>\n",
       "      <td>0.136561</td>\n",
       "      <td>0.129286</td>\n",
       "      <td>0.208963</td>\n",
       "      <td>0.081914</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>0.082584</td>\n",
       "      <td>0.104724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-659</th>\n",
       "      <td>0.100349</td>\n",
       "      <td>0.454765</td>\n",
       "      <td>0.097759</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.294197</td>\n",
       "      <td>0.030203</td>\n",
       "      <td>0.333078</td>\n",
       "      <td>0.159514</td>\n",
       "      <td>0.185337</td>\n",
       "      <td>0.243824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051288</td>\n",
       "      <td>0.289748</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.312011</td>\n",
       "      <td>0.152386</td>\n",
       "      <td>0.189811</td>\n",
       "      <td>0.243346</td>\n",
       "      <td>0.236542</td>\n",
       "      <td>0.255064</td>\n",
       "      <td>0.264170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c-63</th>\n",
       "      <td>0.246456</td>\n",
       "      <td>0.616351</td>\n",
       "      <td>0.042053</td>\n",
       "      <td>0.294197</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018447</td>\n",
       "      <td>0.758814</td>\n",
       "      <td>0.278103</td>\n",
       "      <td>0.523874</td>\n",
       "      <td>0.819012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.713567</td>\n",
       "      <td>0.320842</td>\n",
       "      <td>0.751803</td>\n",
       "      <td>0.481840</td>\n",
       "      <td>0.271333</td>\n",
       "      <td>0.883760</td>\n",
       "      <td>0.542388</td>\n",
       "      <td>0.865735</td>\n",
       "      <td>0.894492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-87</th>\n",
       "      <td>0.222741</td>\n",
       "      <td>0.300633</td>\n",
       "      <td>0.208963</td>\n",
       "      <td>0.189811</td>\n",
       "      <td>0.271333</td>\n",
       "      <td>0.215913</td>\n",
       "      <td>0.188230</td>\n",
       "      <td>0.039990</td>\n",
       "      <td>0.083134</td>\n",
       "      <td>0.191933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022273</td>\n",
       "      <td>0.243812</td>\n",
       "      <td>0.206068</td>\n",
       "      <td>0.169288</td>\n",
       "      <td>0.111634</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.247889</td>\n",
       "      <td>0.109345</td>\n",
       "      <td>0.274950</td>\n",
       "      <td>0.271194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c-54</th>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.601226</td>\n",
       "      <td>0.081914</td>\n",
       "      <td>0.243346</td>\n",
       "      <td>0.883760</td>\n",
       "      <td>0.026074</td>\n",
       "      <td>0.752234</td>\n",
       "      <td>0.285187</td>\n",
       "      <td>0.542073</td>\n",
       "      <td>0.833677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.697572</td>\n",
       "      <td>0.350468</td>\n",
       "      <td>0.769831</td>\n",
       "      <td>0.468817</td>\n",
       "      <td>0.247889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.509209</td>\n",
       "      <td>0.869441</td>\n",
       "      <td>0.890666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-400</th>\n",
       "      <td>0.024834</td>\n",
       "      <td>0.405114</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>0.236542</td>\n",
       "      <td>0.542388</td>\n",
       "      <td>0.070907</td>\n",
       "      <td>0.449045</td>\n",
       "      <td>0.238624</td>\n",
       "      <td>0.209975</td>\n",
       "      <td>0.500912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028338</td>\n",
       "      <td>0.575485</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.348739</td>\n",
       "      <td>0.394370</td>\n",
       "      <td>0.109345</td>\n",
       "      <td>0.509209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522862</td>\n",
       "      <td>0.505578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c-93</th>\n",
       "      <td>0.296697</td>\n",
       "      <td>0.625284</td>\n",
       "      <td>0.082584</td>\n",
       "      <td>0.255064</td>\n",
       "      <td>0.865735</td>\n",
       "      <td>0.038032</td>\n",
       "      <td>0.758965</td>\n",
       "      <td>0.278014</td>\n",
       "      <td>0.514965</td>\n",
       "      <td>0.809979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046762</td>\n",
       "      <td>0.694274</td>\n",
       "      <td>0.333382</td>\n",
       "      <td>0.749618</td>\n",
       "      <td>0.505938</td>\n",
       "      <td>0.274950</td>\n",
       "      <td>0.869441</td>\n",
       "      <td>0.522862</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c-94</th>\n",
       "      <td>0.288032</td>\n",
       "      <td>0.611929</td>\n",
       "      <td>0.104724</td>\n",
       "      <td>0.264170</td>\n",
       "      <td>0.894492</td>\n",
       "      <td>0.041246</td>\n",
       "      <td>0.764285</td>\n",
       "      <td>0.291237</td>\n",
       "      <td>0.513594</td>\n",
       "      <td>0.823444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035222</td>\n",
       "      <td>0.706918</td>\n",
       "      <td>0.354900</td>\n",
       "      <td>0.765055</td>\n",
       "      <td>0.488380</td>\n",
       "      <td>0.271194</td>\n",
       "      <td>0.890666</td>\n",
       "      <td>0.505578</td>\n",
       "      <td>0.886620</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226 rows Ã— 226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          g-280     g-140     g-202     g-659      c-63     g-573     g-386  \\\n",
       "g-280  1.000000  0.404634  0.004110  0.100349  0.246456  0.127427  0.236207   \n",
       "g-140  0.404634  1.000000  0.051667  0.454765  0.616351  0.142014  0.661661   \n",
       "g-202  0.004110  0.051667  1.000000  0.097759  0.042053  0.031437  0.121548   \n",
       "g-659  0.100349  0.454765  0.097759  1.000000  0.294197  0.030203  0.333078   \n",
       "c-63   0.246456  0.616351  0.042053  0.294197  1.000000  0.018447  0.758814   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "g-87   0.222741  0.300633  0.208963  0.189811  0.271333  0.215913  0.188230   \n",
       "c-54   0.261400  0.601226  0.081914  0.243346  0.883760  0.026074  0.752234   \n",
       "g-400  0.024834  0.405114  0.009181  0.236542  0.542388  0.070907  0.449045   \n",
       "c-93   0.296697  0.625284  0.082584  0.255064  0.865735  0.038032  0.758965   \n",
       "c-94   0.288032  0.611929  0.104724  0.264170  0.894492  0.041246  0.764285   \n",
       "\n",
       "          g-653      g-61      c-98  ...     g-609     g-186     g-587  \\\n",
       "g-280  0.137636  0.084422  0.243840  ...  0.017336  0.161116  0.276532   \n",
       "g-140  0.261368  0.427394  0.591029  ...  0.130393  0.528228  0.272609   \n",
       "g-202  0.049936  0.071900  0.163773  ...  0.007337  0.029867  0.017984   \n",
       "g-659  0.159514  0.185337  0.243824  ...  0.051288  0.289748  0.000624   \n",
       "c-63   0.278103  0.523874  0.819012  ...  0.061056  0.713567  0.320842   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "g-87   0.039990  0.083134  0.191933  ...  0.022273  0.243812  0.206068   \n",
       "c-54   0.285187  0.542073  0.833677  ...  0.043367  0.697572  0.350468   \n",
       "g-400  0.238624  0.209975  0.500912  ...  0.028338  0.575485  0.044900   \n",
       "c-93   0.278014  0.514965  0.809979  ...  0.046762  0.694274  0.333382   \n",
       "c-94   0.291237  0.513594  0.823444  ...  0.035222  0.706918  0.354900   \n",
       "\n",
       "          g-629     g-702      g-87      c-54     g-400      c-93      c-94  \n",
       "g-280  0.245769  0.086189  0.222741  0.261400  0.024834  0.296697  0.288032  \n",
       "g-140  0.645036  0.431011  0.300633  0.601226  0.405114  0.625284  0.611929  \n",
       "g-202  0.136561  0.129286  0.208963  0.081914  0.009181  0.082584  0.104724  \n",
       "g-659  0.312011  0.152386  0.189811  0.243346  0.236542  0.255064  0.264170  \n",
       "c-63   0.751803  0.481840  0.271333  0.883760  0.542388  0.865735  0.894492  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "g-87   0.169288  0.111634  1.000000  0.247889  0.109345  0.274950  0.271194  \n",
       "c-54   0.769831  0.468817  0.247889  1.000000  0.509209  0.869441  0.890666  \n",
       "g-400  0.348739  0.394370  0.109345  0.509209  1.000000  0.522862  0.505578  \n",
       "c-93   0.749618  0.505938  0.274950  0.869441  0.522862  1.000000  0.886620  \n",
       "c-94   0.765055  0.488380  0.271194  0.890666  0.505578  0.886620  1.000000  \n",
       "\n",
       "[226 rows x 226 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrM = np.abs(xTrain[corrVars].corr())### working with absolute correlations\n",
    "corrM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropCorrelatedVars(corrM,cutoff):\n",
    "    '''\n",
    "    drop correlated vars having cor > cutoff, of the 2 variables with high corr\n",
    "    will drop the one with the lower importance\n",
    "    corrM corrlation matrix in order of least important to most imp variables\n",
    "    cutoff Threshold\n",
    "    '''\n",
    "    \n",
    "    dropVars=[]\n",
    "    for i in range(corrM.shape[0]):\n",
    "        for j in range(i+1, corrM.shape[0]):\n",
    "\n",
    "            if corrM.iloc[i,j] >cutoff:\n",
    "                #print(f' corr of {corrM.columns[i],corrM.columns[j]} is {corrM.iloc[i,j]}')\n",
    "                dropVars.append(corrM.columns[j])\n",
    "    return list(set(dropVars))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "results ={}\n",
    "orgLoss = 0.016921     ### this is what we got at dropping 240 variables\n",
    "for cutoff in np.arange(0.95,0.3,-0.05):\n",
    "    droppedVars = dropCorrelatedVars(corrM,cutoff=cutoff)    \n",
    "    remainingVars = list(set(corrVars).difference(set(droppedVars)))\n",
    "\n",
    "    clf = MultiOutputClassifier(xgMod).fit(xTrain[remainingVars], yTrain)\n",
    "    #get loss\n",
    "    \n",
    "    trainPreds = clf.predict_proba(xTrain[remainingVars])\n",
    "    trainPreds = np.array(trainPreds)[:,:,1].T\n",
    "    trainLoss = log_loss(np.ravel(yTrain), np.ravel(trainPreds))\n",
    "    \n",
    "    validPredsIn = clf.predict_proba(xValid[remainingVars])\n",
    "    validPredsIn = np.array(validPredsIn)[:,:,1].T\n",
    "    testLoss = log_loss(np.ravel(yValid), np.ravel(validPredsIn))\n",
    " \n",
    "    results[f'CutOff_{np.round(cutoff,2)}'] = (len(droppedVars),trainLoss,testLoss,(testLoss-orgLoss))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cutOff</th>\n",
       "      <th>numDropped</th>\n",
       "      <th>TrainLoss</th>\n",
       "      <th>TestLoss</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CutOff_0.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.016940</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CutOff_0.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.016926</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CutOff_0.85</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>0.017051</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CutOff_0.8</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.008253</td>\n",
       "      <td>0.017326</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CutOff_0.75</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.008295</td>\n",
       "      <td>0.017291</td>\n",
       "      <td>0.000370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CutOff_0.7</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>0.017337</td>\n",
       "      <td>0.000416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CutOff_0.65</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>0.017406</td>\n",
       "      <td>0.000485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CutOff_0.6</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.008864</td>\n",
       "      <td>0.017434</td>\n",
       "      <td>0.000513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CutOff_0.55</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.009190</td>\n",
       "      <td>0.017554</td>\n",
       "      <td>0.000633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CutOff_0.5</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.009515</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.000704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CutOff_0.45</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>0.017686</td>\n",
       "      <td>0.000765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CutOff_0.4</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.010462</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>0.000784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CutOff_0.35</td>\n",
       "      <td>159.0</td>\n",
       "      <td>0.011405</td>\n",
       "      <td>0.017878</td>\n",
       "      <td>0.000957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cutOff  numDropped  TrainLoss  TestLoss      Diff\n",
       "0   CutOff_0.95         0.0   0.007890  0.016940  0.000019\n",
       "1    CutOff_0.9         7.0   0.007944  0.016926  0.000005\n",
       "2   CutOff_0.85        24.0   0.008117  0.017051  0.000130\n",
       "3    CutOff_0.8        32.0   0.008253  0.017326  0.000405\n",
       "4   CutOff_0.75        37.0   0.008295  0.017291  0.000370\n",
       "5    CutOff_0.7        41.0   0.008338  0.017337  0.000416\n",
       "6   CutOff_0.65        56.0   0.008519  0.017406  0.000485\n",
       "7    CutOff_0.6        79.0   0.008864  0.017434  0.000513\n",
       "8   CutOff_0.55        96.0   0.009190  0.017554  0.000633\n",
       "9    CutOff_0.5       109.0   0.009515  0.017625  0.000704\n",
       "10  CutOff_0.45       126.0   0.010008  0.017686  0.000765\n",
       "11   CutOff_0.4       139.0   0.010462  0.017705  0.000784\n",
       "12  CutOff_0.35       159.0   0.011405  0.017878  0.000957"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corResults=pd.DataFrame(results).T.reset_index()\n",
    "corResults.columns = ['cutOff','numDropped','TrainLoss','TestLoss','Diff']\n",
    "corResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'loss vs correlation cutOff'}>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corResults['TestLoss'].plot(title='loss vs correlation cutOff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop correlated variables upto 0.9 cutoff, since there is decrease in performance after that level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "droppedVars = dropCorrelatedVars(corrM,cutoff=0.9) ### saving list upto    \n",
    "finVars =  set(corrVars).difference(set(droppedVars))\n",
    "pd.DataFrame(finVars).to_csv('../dumps/finVars_1ModelXg.csv')\n",
    "len(finVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "finVars = pd.read_csv('../dumps/finVars_1ModelXg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "finVars =list(finVars.iloc[:,1].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we need to run grid search on these vars\n",
    "\n",
    "### We can cv over top 5 models picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridParamTune(gridParams,params,Drop=1,verbose=1):\n",
    "    '''\n",
    "    Xgboost Param tuning\n",
    "    Author: Taran\n",
    "    \n",
    "    Given a params dictionary this function implements Grid and random search\n",
    "    The cv object can be replaced for any other model\n",
    "    \n",
    "    Args - dTrain matrix,\n",
    "            gridParams A parameters dictinary with candidate search space\n",
    "            Drop Rate - for  Random search [0,1]\n",
    "            verbose 0 0r 1 \n",
    "            params  ---not to be tuned parameters\n",
    "            EvalMetric  -- takes 1 eval metric\n",
    "    Output\n",
    "    Returns a df with results for each params\n",
    "    Additional dependency itertools\n",
    "    \n",
    "    '''\n",
    "    import itertools\n",
    "    #paramers passed\n",
    "    paramNames = list(gridParams.keys())\n",
    "    \n",
    "    results = []\n",
    "    ### iterate over all combinations\n",
    "    for row in itertools.product(*gridParams.values()):\n",
    "        ### random search threshold\n",
    "        if np.random.random(1)[0] > Drop:\n",
    "            continue \n",
    "        \n",
    "        # insert values into param dict\n",
    "        for i in range(len(row)):\n",
    "            params[paramNames[i]] = row[i]\n",
    "        \n",
    "        xgMod = XGBClassifier(**params,random_state=SEED)\n",
    "\n",
    "        #get results \n",
    "        clf = MultiOutputClassifier(xgMod).fit(xTrain[finVars], yTrain)\n",
    "        #get loss\n",
    "    \n",
    "        trainPreds = clf.predict_proba(xTrain[finVars])\n",
    "        trainPreds = np.array(trainPreds)[:,:,1].T\n",
    "        trainLoss = log_loss(np.ravel(yTrain), np.ravel(trainPreds))\n",
    "\n",
    "        validPredsIn = clf.predict_proba(xValid[finVars])\n",
    "        validPredsIn = np.array(validPredsIn)[:,:,1].T\n",
    "        testLoss = log_loss(np.ravel(yValid), np.ravel(validPredsIn))\n",
    "\n",
    "\n",
    "        overfit = ( (testLoss/trainLoss)-1)*100\n",
    "        \n",
    "        #unlist\n",
    "        tempResults=[list(params.values())[1:],trainLoss,testLoss,overfit]      \n",
    "\n",
    "        results.append(list(itertools.chain.from_iterable(i if isinstance(i, list) else [i] for i in tempResults)))\n",
    "         \n",
    "#     colNames=[paramNames,'trainLoss','testLoss','overfit']              \n",
    "#     df = pd.DataFrame(results,columns=list(itertools.chain.from_iterable(i if isinstance(i, list) else [i] for i in colNames)))\n",
    "                  \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NumRows =xTrain.shape[0]\n",
    "gridParams = {\n",
    "   \n",
    "    'max_depth' : [8,10],### making tree shallower by max leaves\n",
    "    'eta' : [0.05],\n",
    "    'colsample_bytree' : [0.4,0.7],\n",
    "    'min_child_weight' : [int(NumRows*0.001),int(NumRows*0.003)],\n",
    "    'gamma': [1,5],\n",
    "    'max_leaves': [16,24], ## to constraint tree to be shallower,and deeper \n",
    "    'max_delta_step':[1,5]\n",
    "}\n",
    "\n",
    "\n",
    "params = {\n",
    "     'tree_method':'gpu_hist',\n",
    "    'grow_policy': 'lossguide',\n",
    "    'objective':'binary:logistic',\n",
    "     'n_estimators': 1000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "     'tree_method':'gpu_hist',\n",
    "    'grow_policy': 'lossguide',\n",
    "    'objective':'binary:logistic',\n",
    "     'n_estimators': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6h 4min 55s, sys: 1min 27s, total: 6h 6min 22s\n",
      "Wall time: 4h 39min 5s\n"
     ]
    }
   ],
   "source": [
    "%time paramResults=GridParamTune(gridParams=gridParams,params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  1,\n",
       "  16,\n",
       "  1,\n",
       "  0.007773869885477753,\n",
       "  0.016961420835653076,\n",
       "  118.18503635285236],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  1,\n",
       "  16,\n",
       "  5,\n",
       "  0.007759219058403552,\n",
       "  0.0169508169857213,\n",
       "  118.46034836924564],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  1,\n",
       "  24,\n",
       "  1,\n",
       "  0.007775756257269503,\n",
       "  0.016943530823206612,\n",
       "  117.90203116727352],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  1,\n",
       "  24,\n",
       "  5,\n",
       "  0.007758980498501979,\n",
       "  0.016948938846716482,\n",
       "  118.44285921312476],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  5,\n",
       "  16,\n",
       "  1,\n",
       "  0.013371439502242979,\n",
       "  0.016528031218714608,\n",
       "  23.606970034468834],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  5,\n",
       "  16,\n",
       "  5,\n",
       "  0.013344194550292288,\n",
       "  0.01653065662621395,\n",
       "  23.879013933080493],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  5,\n",
       "  24,\n",
       "  1,\n",
       "  0.013371439502242979,\n",
       "  0.016528031218714608,\n",
       "  23.606970034468834],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  5,\n",
       "  24,\n",
       "  5,\n",
       "  0.01334680350576194,\n",
       "  0.016530045730668574,\n",
       "  23.850221691900984],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  1,\n",
       "  16,\n",
       "  1,\n",
       "  0.015060919561402934,\n",
       "  0.017858697719746644,\n",
       "  18.5764099392288],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  1,\n",
       "  16,\n",
       "  5,\n",
       "  0.015009128525414406,\n",
       "  0.01783086993022698,\n",
       "  18.80016817788337],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  1,\n",
       "  24,\n",
       "  1,\n",
       "  0.015060919561402934,\n",
       "  0.017858697719746644,\n",
       "  18.5764099392288],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  1,\n",
       "  24,\n",
       "  5,\n",
       "  0.015009128525414406,\n",
       "  0.01783086993022698,\n",
       "  18.80016817788337],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  5,\n",
       "  16,\n",
       "  1,\n",
       "  0.017050151637471856,\n",
       "  0.017779723279579638,\n",
       "  4.278974507794819],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  5,\n",
       "  16,\n",
       "  5,\n",
       "  0.01702817731107515,\n",
       "  0.017764147953506972,\n",
       "  4.322075281381665],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  5,\n",
       "  24,\n",
       "  1,\n",
       "  0.017050151637471856,\n",
       "  0.017779723279579638,\n",
       "  4.278974507794819],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  5,\n",
       "  24,\n",
       "  5,\n",
       "  0.01702817731107515,\n",
       "  0.017764147953506972,\n",
       "  4.322075281381665],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  1,\n",
       "  16,\n",
       "  1,\n",
       "  0.007685541123446963,\n",
       "  0.016969060355712374,\n",
       "  120.7920051841679],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  1,\n",
       "  16,\n",
       "  5,\n",
       "  0.007665319681575772,\n",
       "  0.016972784950282295,\n",
       "  121.42305416273484],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  1,\n",
       "  24,\n",
       "  1,\n",
       "  0.0076796434444843945,\n",
       "  0.016979434236208554,\n",
       "  121.0966480273687],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  1,\n",
       "  24,\n",
       "  5,\n",
       "  0.007664005450296133,\n",
       "  0.01696610175906561,\n",
       "  121.37382167975952],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  5,\n",
       "  16,\n",
       "  1,\n",
       "  0.013245635207942323,\n",
       "  0.016530483544452364,\n",
       "  24.799477601046902],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  5,\n",
       "  16,\n",
       "  5,\n",
       "  0.013198913500913317,\n",
       "  0.01653819443955843,\n",
       "  25.29966529755685],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  5,\n",
       "  24,\n",
       "  1,\n",
       "  0.013239980188568437,\n",
       "  0.016531962902858275,\n",
       "  24.863954986369063],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  5,\n",
       "  24,\n",
       "  5,\n",
       "  0.01319805355146498,\n",
       "  0.016536424090174938,\n",
       "  25.294415768902525],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  1,\n",
       "  16,\n",
       "  1,\n",
       "  0.014953518216184309,\n",
       "  0.01786347721293946,\n",
       "  19.46002910275442],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  1,\n",
       "  16,\n",
       "  5,\n",
       "  0.014892620463780572,\n",
       "  0.01783049773371547,\n",
       "  19.7270673558084],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  1,\n",
       "  24,\n",
       "  1,\n",
       "  0.014953518216184309,\n",
       "  0.01786347721293946,\n",
       "  19.46002910275442],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  1,\n",
       "  24,\n",
       "  5,\n",
       "  0.014891185834257107,\n",
       "  0.01783079624834997,\n",
       "  19.740606603204824],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  5,\n",
       "  16,\n",
       "  1,\n",
       "  0.016985154685585598,\n",
       "  0.01776558408783528,\n",
       "  4.594773593154233],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  5,\n",
       "  16,\n",
       "  5,\n",
       "  0.016962726320939887,\n",
       "  0.017755431022039414,\n",
       "  4.673215178393586],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  5,\n",
       "  24,\n",
       "  1,\n",
       "  0.016985154685585598,\n",
       "  0.01776558408783528,\n",
       "  4.594773593154233],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  8,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  5,\n",
       "  24,\n",
       "  5,\n",
       "  0.016962726320939887,\n",
       "  0.017755431022039414,\n",
       "  4.673215178393586],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  1,\n",
       "  16,\n",
       "  1,\n",
       "  0.007777297540694171,\n",
       "  0.016943523715450277,\n",
       "  117.8587565512887],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  1,\n",
       "  16,\n",
       "  5,\n",
       "  0.0077599196764452545,\n",
       "  0.016949148860550625,\n",
       "  118.41912761028564],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  1,\n",
       "  24,\n",
       "  1,\n",
       "  0.007769715440167432,\n",
       "  0.01694712816166947,\n",
       "  118.11774565201154],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  1,\n",
       "  24,\n",
       "  5,\n",
       "  0.007754370906271524,\n",
       "  0.01696793769158713,\n",
       "  118.81772095611166],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  5,\n",
       "  16,\n",
       "  1,\n",
       "  0.013357322115449616,\n",
       "  0.016525798082718743,\n",
       "  23.72089210609318],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  5,\n",
       "  16,\n",
       "  5,\n",
       "  0.013346474014267288,\n",
       "  0.016528424837978072,\n",
       "  23.84113452219141],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  5,\n",
       "  24,\n",
       "  1,\n",
       "  0.013361012319180098,\n",
       "  0.01652559644695702,\n",
       "  23.685212259209408],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  16,\n",
       "  5,\n",
       "  24,\n",
       "  5,\n",
       "  0.013346474014267288,\n",
       "  0.016528424837978072,\n",
       "  23.84113452219141],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  1,\n",
       "  16,\n",
       "  1,\n",
       "  0.015064342549022815,\n",
       "  0.017860976652677298,\n",
       "  18.56459446904899],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  1,\n",
       "  16,\n",
       "  5,\n",
       "  0.015005199574648508,\n",
       "  0.017836371779197193,\n",
       "  18.86794101247404],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  1,\n",
       "  24,\n",
       "  1,\n",
       "  0.015064342549022815,\n",
       "  0.017860976652677298,\n",
       "  18.56459446904899],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  1,\n",
       "  24,\n",
       "  5,\n",
       "  0.01500545063306757,\n",
       "  0.01783856376960755,\n",
       "  18.88056017655768],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  5,\n",
       "  16,\n",
       "  1,\n",
       "  0.017050151637471856,\n",
       "  0.017779723279579638,\n",
       "  4.278974507794819],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  5,\n",
       "  16,\n",
       "  5,\n",
       "  0.01702817731107515,\n",
       "  0.017764147953506972,\n",
       "  4.322075281381665],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  5,\n",
       "  24,\n",
       "  1,\n",
       "  0.017050151637471856,\n",
       "  0.017779723279579638,\n",
       "  4.278974507794819],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.4,\n",
       "  50,\n",
       "  5,\n",
       "  24,\n",
       "  5,\n",
       "  0.01702817731107515,\n",
       "  0.017764147953506972,\n",
       "  4.322075281381665],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  1,\n",
       "  16,\n",
       "  1,\n",
       "  0.007679382876096171,\n",
       "  0.016985805485290932,\n",
       "  121.18711567518167],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  1,\n",
       "  16,\n",
       "  5,\n",
       "  0.007658443474300517,\n",
       "  0.01695980230635678,\n",
       "  121.45234032566651],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  1,\n",
       "  24,\n",
       "  1,\n",
       "  0.007675859006430068,\n",
       "  0.016976680270799193,\n",
       "  121.16977730541723],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  1,\n",
       "  24,\n",
       "  5,\n",
       "  0.007653723185959007,\n",
       "  0.016968416410009162,\n",
       "  121.70146473468297],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  5,\n",
       "  16,\n",
       "  1,\n",
       "  0.013219700986912553,\n",
       "  0.01653458441522778,\n",
       "  25.07532834212323],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  5,\n",
       "  16,\n",
       "  5,\n",
       "  0.013179625926893528,\n",
       "  0.016539324501831584,\n",
       "  25.491607983216458],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  5,\n",
       "  24,\n",
       "  1,\n",
       "  0.013220896733041118,\n",
       "  0.01653528313731241,\n",
       "  25.069301055715187],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  16,\n",
       "  5,\n",
       "  24,\n",
       "  5,\n",
       "  0.01319590067975329,\n",
       "  0.016538731492642508,\n",
       "  25.33234292993871],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  1,\n",
       "  16,\n",
       "  1,\n",
       "  0.01495307778025887,\n",
       "  0.01786410316681226,\n",
       "  19.46773386276732],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  1,\n",
       "  16,\n",
       "  5,\n",
       "  0.014891172478621309,\n",
       "  0.017833440381072853,\n",
       "  19.7584703734756],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  1,\n",
       "  24,\n",
       "  1,\n",
       "  0.01495307778025887,\n",
       "  0.01786410316681226,\n",
       "  19.46773386276732],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  1,\n",
       "  24,\n",
       "  5,\n",
       "  0.014891215374388464,\n",
       "  0.01783177575493957,\n",
       "  19.746946818112665],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  5,\n",
       "  16,\n",
       "  1,\n",
       "  0.016985154685585598,\n",
       "  0.01776558408783528,\n",
       "  4.594773593154233],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  5,\n",
       "  16,\n",
       "  5,\n",
       "  0.016962726320939887,\n",
       "  0.017755431022039414,\n",
       "  4.673215178393586],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  5,\n",
       "  24,\n",
       "  1,\n",
       "  0.016985154685585598,\n",
       "  0.01776558408783528,\n",
       "  4.594773593154233],\n",
       " ['lossguide',\n",
       "  'binary:logistic',\n",
       "  1000,\n",
       "  10,\n",
       "  0.05,\n",
       "  0.7,\n",
       "  50,\n",
       "  5,\n",
       "  24,\n",
       "  5,\n",
       "  0.016962726320939887,\n",
       "  0.017755431022039414,\n",
       "  4.673215178393586]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>growth</th>\n",
       "      <th>obj</th>\n",
       "      <th>rounds</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>eta</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>max_delta_step</th>\n",
       "      <th>trainLoss</th>\n",
       "      <th>testLoss</th>\n",
       "      <th>overfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007774</td>\n",
       "      <td>0.016961</td>\n",
       "      <td>118.185036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007759</td>\n",
       "      <td>0.016951</td>\n",
       "      <td>118.460348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>117.902031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007759</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>118.442859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013371</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>23.606970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014891</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>19.746947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.017766</td>\n",
       "      <td>4.594774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>4.673215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.017766</td>\n",
       "      <td>4.594774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>4.673215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       growth              obj  rounds  max_depth   eta  colsample_bytree  \\\n",
       "0   lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "1   lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "2   lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "3   lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "4   lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "..        ...              ...     ...        ...   ...               ...   \n",
       "59  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "60  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "61  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "62  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "63  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "\n",
       "    min_child_weight  gamma  max_leaves  max_delta_step  trainLoss  testLoss  \\\n",
       "0                 16      1          16               1   0.007774  0.016961   \n",
       "1                 16      1          16               5   0.007759  0.016951   \n",
       "2                 16      1          24               1   0.007776  0.016944   \n",
       "3                 16      1          24               5   0.007759  0.016949   \n",
       "4                 16      5          16               1   0.013371  0.016528   \n",
       "..               ...    ...         ...             ...        ...       ...   \n",
       "59                50      1          24               5   0.014891  0.017832   \n",
       "60                50      5          16               1   0.016985  0.017766   \n",
       "61                50      5          16               5   0.016963  0.017755   \n",
       "62                50      5          24               1   0.016985  0.017766   \n",
       "63                50      5          24               5   0.016963  0.017755   \n",
       "\n",
       "       overfit  \n",
       "0   118.185036  \n",
       "1   118.460348  \n",
       "2   117.902031  \n",
       "3   118.442859  \n",
       "4    23.606970  \n",
       "..         ...  \n",
       "59   19.746947  \n",
       "60    4.594774  \n",
       "61    4.673215  \n",
       "62    4.594774  \n",
       "63    4.673215  \n",
       "\n",
       "[64 rows x 13 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "colNames=['growth','obj','rounds',list(gridParams.keys()),'trainLoss','testLoss','overfit']              \n",
    "df = pd.DataFrame(paramResults,columns=list(itertools.chain.from_iterable(i if isinstance(i, list) else [i] for i in colNames)))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>growth</th>\n",
       "      <th>obj</th>\n",
       "      <th>rounds</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>eta</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>max_delta_step</th>\n",
       "      <th>trainLoss</th>\n",
       "      <th>testLoss</th>\n",
       "      <th>overfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013361</td>\n",
       "      <td>0.016526</td>\n",
       "      <td>23.685212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013357</td>\n",
       "      <td>0.016526</td>\n",
       "      <td>23.720892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013371</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>23.606970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013371</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>23.606970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>23.841135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>23.841135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013347</td>\n",
       "      <td>0.016530</td>\n",
       "      <td>23.850222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013246</td>\n",
       "      <td>0.016530</td>\n",
       "      <td>24.799478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>0.016531</td>\n",
       "      <td>23.879014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>0.016532</td>\n",
       "      <td>24.863955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>0.016535</td>\n",
       "      <td>25.075328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013221</td>\n",
       "      <td>0.016535</td>\n",
       "      <td>25.069301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013198</td>\n",
       "      <td>0.016536</td>\n",
       "      <td>25.294416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013199</td>\n",
       "      <td>0.016538</td>\n",
       "      <td>25.299665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013196</td>\n",
       "      <td>0.016539</td>\n",
       "      <td>25.332343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013180</td>\n",
       "      <td>0.016539</td>\n",
       "      <td>25.491608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007777</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>117.858757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>117.902031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>0.016947</td>\n",
       "      <td>118.117746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007759</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>118.442859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       growth              obj  rounds  max_depth   eta  colsample_bytree  \\\n",
       "38  lossguide  binary:logistic    1000         10  0.05               0.4   \n",
       "36  lossguide  binary:logistic    1000         10  0.05               0.4   \n",
       "4   lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "6   lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "39  lossguide  binary:logistic    1000         10  0.05               0.4   \n",
       "37  lossguide  binary:logistic    1000         10  0.05               0.4   \n",
       "7   lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "20  lossguide  binary:logistic    1000          8  0.05               0.7   \n",
       "5   lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "22  lossguide  binary:logistic    1000          8  0.05               0.7   \n",
       "52  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "54  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "23  lossguide  binary:logistic    1000          8  0.05               0.7   \n",
       "21  lossguide  binary:logistic    1000          8  0.05               0.7   \n",
       "55  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "53  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "32  lossguide  binary:logistic    1000         10  0.05               0.4   \n",
       "2   lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "34  lossguide  binary:logistic    1000         10  0.05               0.4   \n",
       "3   lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "\n",
       "    min_child_weight  gamma  max_leaves  max_delta_step  trainLoss  testLoss  \\\n",
       "38                16      5          24               1   0.013361  0.016526   \n",
       "36                16      5          16               1   0.013357  0.016526   \n",
       "4                 16      5          16               1   0.013371  0.016528   \n",
       "6                 16      5          24               1   0.013371  0.016528   \n",
       "39                16      5          24               5   0.013346  0.016528   \n",
       "37                16      5          16               5   0.013346  0.016528   \n",
       "7                 16      5          24               5   0.013347  0.016530   \n",
       "20                16      5          16               1   0.013246  0.016530   \n",
       "5                 16      5          16               5   0.013344  0.016531   \n",
       "22                16      5          24               1   0.013240  0.016532   \n",
       "52                16      5          16               1   0.013220  0.016535   \n",
       "54                16      5          24               1   0.013221  0.016535   \n",
       "23                16      5          24               5   0.013198  0.016536   \n",
       "21                16      5          16               5   0.013199  0.016538   \n",
       "55                16      5          24               5   0.013196  0.016539   \n",
       "53                16      5          16               5   0.013180  0.016539   \n",
       "32                16      1          16               1   0.007777  0.016944   \n",
       "2                 16      1          24               1   0.007776  0.016944   \n",
       "34                16      1          24               1   0.007770  0.016947   \n",
       "3                 16      1          24               5   0.007759  0.016949   \n",
       "\n",
       "       overfit  \n",
       "38   23.685212  \n",
       "36   23.720892  \n",
       "4    23.606970  \n",
       "6    23.606970  \n",
       "39   23.841135  \n",
       "37   23.841135  \n",
       "7    23.850222  \n",
       "20   24.799478  \n",
       "5    23.879014  \n",
       "22   24.863955  \n",
       "52   25.075328  \n",
       "54   25.069301  \n",
       "23   25.294416  \n",
       "21   25.299665  \n",
       "55   25.332343  \n",
       "53   25.491608  \n",
       "32  117.858757  \n",
       "2   117.902031  \n",
       "34  118.117746  \n",
       "3   118.442859  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(['testLoss']).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>growth</th>\n",
       "      <th>obj</th>\n",
       "      <th>rounds</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>eta</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>max_delta_step</th>\n",
       "      <th>trainLoss</th>\n",
       "      <th>testLoss</th>\n",
       "      <th>overfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>0.017780</td>\n",
       "      <td>4.278975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>0.017780</td>\n",
       "      <td>4.278975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>0.017780</td>\n",
       "      <td>4.278975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>0.017780</td>\n",
       "      <td>4.278975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0.017028</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>4.322075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.017028</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>4.322075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0.017028</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>4.322075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.017028</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>4.322075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.017766</td>\n",
       "      <td>4.594774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.017766</td>\n",
       "      <td>4.594774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.017766</td>\n",
       "      <td>4.594774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.017766</td>\n",
       "      <td>4.594774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>4.673215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>4.673215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>4.673215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>4.673215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.017861</td>\n",
       "      <td>18.564594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.017861</td>\n",
       "      <td>18.564594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015061</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>18.576410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015061</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>18.576410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       growth              obj  rounds  max_depth   eta  colsample_bytree  \\\n",
       "46  lossguide  binary:logistic    1000         10  0.05               0.4   \n",
       "44  lossguide  binary:logistic    1000         10  0.05               0.4   \n",
       "12  lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "14  lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "47  lossguide  binary:logistic    1000         10  0.05               0.4   \n",
       "45  lossguide  binary:logistic    1000         10  0.05               0.4   \n",
       "15  lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "13  lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "60  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "62  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "30  lossguide  binary:logistic    1000          8  0.05               0.7   \n",
       "28  lossguide  binary:logistic    1000          8  0.05               0.7   \n",
       "31  lossguide  binary:logistic    1000          8  0.05               0.7   \n",
       "61  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "29  lossguide  binary:logistic    1000          8  0.05               0.7   \n",
       "63  lossguide  binary:logistic    1000         10  0.05               0.7   \n",
       "40  lossguide  binary:logistic    1000         10  0.05               0.4   \n",
       "42  lossguide  binary:logistic    1000         10  0.05               0.4   \n",
       "10  lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "8   lossguide  binary:logistic    1000          8  0.05               0.4   \n",
       "\n",
       "    min_child_weight  gamma  max_leaves  max_delta_step  trainLoss  testLoss  \\\n",
       "46                50      5          24               1   0.017050  0.017780   \n",
       "44                50      5          16               1   0.017050  0.017780   \n",
       "12                50      5          16               1   0.017050  0.017780   \n",
       "14                50      5          24               1   0.017050  0.017780   \n",
       "47                50      5          24               5   0.017028  0.017764   \n",
       "45                50      5          16               5   0.017028  0.017764   \n",
       "15                50      5          24               5   0.017028  0.017764   \n",
       "13                50      5          16               5   0.017028  0.017764   \n",
       "60                50      5          16               1   0.016985  0.017766   \n",
       "62                50      5          24               1   0.016985  0.017766   \n",
       "30                50      5          24               1   0.016985  0.017766   \n",
       "28                50      5          16               1   0.016985  0.017766   \n",
       "31                50      5          24               5   0.016963  0.017755   \n",
       "61                50      5          16               5   0.016963  0.017755   \n",
       "29                50      5          16               5   0.016963  0.017755   \n",
       "63                50      5          24               5   0.016963  0.017755   \n",
       "40                50      1          16               1   0.015064  0.017861   \n",
       "42                50      1          24               1   0.015064  0.017861   \n",
       "10                50      1          24               1   0.015061  0.017859   \n",
       "8                 50      1          16               1   0.015061  0.017859   \n",
       "\n",
       "      overfit  \n",
       "46   4.278975  \n",
       "44   4.278975  \n",
       "12   4.278975  \n",
       "14   4.278975  \n",
       "47   4.322075  \n",
       "45   4.322075  \n",
       "15   4.322075  \n",
       "13   4.322075  \n",
       "60   4.594774  \n",
       "62   4.594774  \n",
       "30   4.594774  \n",
       "28   4.594774  \n",
       "31   4.673215  \n",
       "61   4.673215  \n",
       "29   4.673215  \n",
       "63   4.673215  \n",
       "40  18.564594  \n",
       "42  18.564594  \n",
       "10  18.576410  \n",
       "8   18.576410  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(['overfit']).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tuning Gamma\n",
    "NumRows =xTrain.shape[0]\n",
    "gridParams = {\n",
    "   \n",
    "    'max_depth' : [10],### making tree shallower by max leaves\n",
    "    'eta' : [0.05],\n",
    "    'colsample_bytree' : [0.4],\n",
    "    'min_child_weight' : [int(NumRows*0.001)],\n",
    "    'gamma': [3,5,8],\n",
    "    'max_leaves': [24], ## to constraint tree to be shallower,and deeper \n",
    "    'max_delta_step':[1],\n",
    "     'n_estimators': [1000,1500,2000]\n",
    "}\n",
    "\n",
    "\n",
    "params = {\n",
    "     'tree_method':'gpu_hist',\n",
    "    'grow_policy': 'lossguide',\n",
    "    'objective':'binary:logistic'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 4min 31s, sys: 14.1 s, total: 1h 4min 45s\n",
      "Wall time: 50min 48s\n"
     ]
    }
   ],
   "source": [
    "%time paramResults2=GridParamTune(gridParams=gridParams,params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj</th>\n",
       "      <th>rounds</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>eta</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>max_delta_step</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>trainLoss</th>\n",
       "      <th>testLoss</th>\n",
       "      <th>overfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.016524</td>\n",
       "      <td>48.152871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.016524</td>\n",
       "      <td>48.152838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.016524</td>\n",
       "      <td>48.152825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.013361</td>\n",
       "      <td>0.016526</td>\n",
       "      <td>23.685212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.013361</td>\n",
       "      <td>0.016526</td>\n",
       "      <td>23.685184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.013361</td>\n",
       "      <td>0.016526</td>\n",
       "      <td>23.685174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>9.846470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>9.846460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>9.846455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         obj           rounds  max_depth   eta  colsample_bytree  \\\n",
       "0  lossguide  binary:logistic         10  0.05               0.4   \n",
       "1  lossguide  binary:logistic         10  0.05               0.4   \n",
       "2  lossguide  binary:logistic         10  0.05               0.4   \n",
       "3  lossguide  binary:logistic         10  0.05               0.4   \n",
       "4  lossguide  binary:logistic         10  0.05               0.4   \n",
       "5  lossguide  binary:logistic         10  0.05               0.4   \n",
       "6  lossguide  binary:logistic         10  0.05               0.4   \n",
       "7  lossguide  binary:logistic         10  0.05               0.4   \n",
       "8  lossguide  binary:logistic         10  0.05               0.4   \n",
       "\n",
       "   min_child_weight  gamma  max_leaves  max_delta_step  n_estimators  \\\n",
       "0                16      3          24               1          1000   \n",
       "1                16      3          24               1          1500   \n",
       "2                16      3          24               1          2000   \n",
       "3                16      5          24               1          1000   \n",
       "4                16      5          24               1          1500   \n",
       "5                16      5          24               1          2000   \n",
       "6                16      8          24               1          1000   \n",
       "7                16      8          24               1          1500   \n",
       "8                16      8          24               1          2000   \n",
       "\n",
       "   trainLoss  testLoss    overfit  \n",
       "0   0.011154  0.016524  48.152871  \n",
       "1   0.011154  0.016524  48.152838  \n",
       "2   0.011154  0.016524  48.152825  \n",
       "3   0.013361  0.016526  23.685212  \n",
       "4   0.013361  0.016526  23.685184  \n",
       "5   0.013361  0.016526  23.685174  \n",
       "6   0.015185  0.016680   9.846470  \n",
       "7   0.015185  0.016680   9.846460  \n",
       "8   0.015185  0.016680   9.846455  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames=['obj','rounds',list(gridParams.keys()),'trainLoss','testLoss','overfit']              \n",
    "df2 = pd.DataFrame(paramResults2,columns=list(itertools.chain.from_iterable(i if isinstance(i, list) else [i] for i in colNames)))\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj</th>\n",
       "      <th>rounds</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>eta</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>max_delta_step</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>trainLoss</th>\n",
       "      <th>testLoss</th>\n",
       "      <th>overfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.016524</td>\n",
       "      <td>48.152871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.016524</td>\n",
       "      <td>48.152838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.016524</td>\n",
       "      <td>48.152825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.013361</td>\n",
       "      <td>0.016526</td>\n",
       "      <td>23.685212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.013361</td>\n",
       "      <td>0.016526</td>\n",
       "      <td>23.685184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.013361</td>\n",
       "      <td>0.016526</td>\n",
       "      <td>23.685174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>9.846470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>9.846460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>9.846455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         obj           rounds  max_depth   eta  colsample_bytree  \\\n",
       "0  lossguide  binary:logistic         10  0.05               0.4   \n",
       "1  lossguide  binary:logistic         10  0.05               0.4   \n",
       "2  lossguide  binary:logistic         10  0.05               0.4   \n",
       "3  lossguide  binary:logistic         10  0.05               0.4   \n",
       "4  lossguide  binary:logistic         10  0.05               0.4   \n",
       "5  lossguide  binary:logistic         10  0.05               0.4   \n",
       "6  lossguide  binary:logistic         10  0.05               0.4   \n",
       "7  lossguide  binary:logistic         10  0.05               0.4   \n",
       "8  lossguide  binary:logistic         10  0.05               0.4   \n",
       "\n",
       "   min_child_weight  gamma  max_leaves  max_delta_step  n_estimators  \\\n",
       "0                16      3          24               1          1000   \n",
       "1                16      3          24               1          1500   \n",
       "2                16      3          24               1          2000   \n",
       "3                16      5          24               1          1000   \n",
       "4                16      5          24               1          1500   \n",
       "5                16      5          24               1          2000   \n",
       "6                16      8          24               1          1000   \n",
       "7                16      8          24               1          1500   \n",
       "8                16      8          24               1          2000   \n",
       "\n",
       "   trainLoss  testLoss    overfit  \n",
       "0   0.011154  0.016524  48.152871  \n",
       "1   0.011154  0.016524  48.152838  \n",
       "2   0.011154  0.016524  48.152825  \n",
       "3   0.013361  0.016526  23.685212  \n",
       "4   0.013361  0.016526  23.685184  \n",
       "5   0.013361  0.016526  23.685174  \n",
       "6   0.015185  0.016680   9.846470  \n",
       "7   0.015185  0.016680   9.846460  \n",
       "8   0.015185  0.016680   9.846455  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.sort_values(['testLoss']).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj</th>\n",
       "      <th>rounds</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>eta</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>max_delta_step</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>trainLoss</th>\n",
       "      <th>testLoss</th>\n",
       "      <th>overfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>9.846455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>9.846460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>9.846470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.013361</td>\n",
       "      <td>0.016526</td>\n",
       "      <td>23.685174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.013361</td>\n",
       "      <td>0.016526</td>\n",
       "      <td>23.685184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.013361</td>\n",
       "      <td>0.016526</td>\n",
       "      <td>23.685212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.016524</td>\n",
       "      <td>48.152825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.016524</td>\n",
       "      <td>48.152838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lossguide</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.016524</td>\n",
       "      <td>48.152871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         obj           rounds  max_depth   eta  colsample_bytree  \\\n",
       "8  lossguide  binary:logistic         10  0.05               0.4   \n",
       "7  lossguide  binary:logistic         10  0.05               0.4   \n",
       "6  lossguide  binary:logistic         10  0.05               0.4   \n",
       "5  lossguide  binary:logistic         10  0.05               0.4   \n",
       "4  lossguide  binary:logistic         10  0.05               0.4   \n",
       "3  lossguide  binary:logistic         10  0.05               0.4   \n",
       "2  lossguide  binary:logistic         10  0.05               0.4   \n",
       "1  lossguide  binary:logistic         10  0.05               0.4   \n",
       "0  lossguide  binary:logistic         10  0.05               0.4   \n",
       "\n",
       "   min_child_weight  gamma  max_leaves  max_delta_step  n_estimators  \\\n",
       "8                16      8          24               1          2000   \n",
       "7                16      8          24               1          1500   \n",
       "6                16      8          24               1          1000   \n",
       "5                16      5          24               1          2000   \n",
       "4                16      5          24               1          1500   \n",
       "3                16      5          24               1          1000   \n",
       "2                16      3          24               1          2000   \n",
       "1                16      3          24               1          1500   \n",
       "0                16      3          24               1          1000   \n",
       "\n",
       "   trainLoss  testLoss    overfit  \n",
       "8   0.015185  0.016680   9.846455  \n",
       "7   0.015185  0.016680   9.846460  \n",
       "6   0.015185  0.016680   9.846470  \n",
       "5   0.013361  0.016526  23.685174  \n",
       "4   0.013361  0.016526  23.685184  \n",
       "3   0.013361  0.016526  23.685212  \n",
       "2   0.011154  0.016524  48.152825  \n",
       "1   0.011154  0.016524  48.152838  \n",
       "0   0.011154  0.016524  48.152871  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.sort_values(['overfit']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final model\n",
    "\n",
    "##### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumRows =xTrain.shape[0]\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    #'num_class':2,\n",
    "    'max_depth' : 10, # for gpu hist\n",
    "    'eta' : 0.05,\n",
    "    'colsample_bytree' : 0.4,\n",
    "    'min_child_weight' : int(NumRows*0.001),\n",
    "    'gamma': 5,\n",
    "    'tree_method':'gpu_hist',\n",
    "    'grow_policy': 'lossguide',\n",
    "    'max_leaves': 24, ## to constraint tree to be shallower\n",
    "    'n_estimators': 1000,\n",
    "    'max_delta_step':1\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "xgMod = XGBClassifier(**params,random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taran/depot/Projects/kaggleDev/venv/lib/python3.6/site-packages/xgboost/sklearn.py:695: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n",
      "/home/taran/depot/Projects/kaggleDev/venv/lib/python3.6/site-packages/xgboost/sklearn.py:695: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n",
      "/home/taran/depot/Projects/kaggleDev/venv/lib/python3.6/site-packages/xgboost/sklearn.py:695: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n",
      "/home/taran/depot/Projects/kaggleDev/venv/lib/python3.6/site-packages/xgboost/sklearn.py:695: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n",
      "/home/taran/depot/Projects/kaggleDev/venv/lib/python3.6/site-packages/xgboost/sklearn.py:695: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 4s, sys: 6.26 s, total: 26min 10s\n",
      "Wall time: 19min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01652559644695702,\n",
       " 0.016779702766843662,\n",
       " 0.016569341983876718,\n",
       " 0.016487815920673277,\n",
       " 0.016571669349384307]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainLosses = []\n",
    "validLosses = []\n",
    "validPreds = []\n",
    "featImp = []\n",
    "for i in range(5):\n",
    "    \n",
    "    trainIds= xAll['sig_id'].sample(frac=0.7,random_state=(SEED +100*i + 10*i +i ))\n",
    "    xTrain = xAll[xAll['sig_id'].isin(trainIds)]\n",
    "    yTrain = yAll[yAll['sig_id'].isin(trainIds)]\n",
    "    xValid = xAll[~xAll['sig_id'].isin(trainIds)]\n",
    "    yValid = yAll[~yAll['sig_id'].isin(trainIds)]\n",
    " \n",
    "    xTrain=xTrain.drop(idList,axis=1)\n",
    "    xValid=xValid.drop(idList,axis=1)\n",
    "    yTrain=yTrain.drop(idList,axis=1)\n",
    "    yValid=yValid.drop(idList,axis=1)\n",
    "                                    \n",
    "                                    \n",
    "    #fit model\n",
    "    clf = MultiOutputClassifier(xgMod).fit(xTrain[finVars], yTrain)\n",
    "    #get loss\n",
    "    trainPreds = clf.predict_proba(xTrain[finVars])\n",
    "    trainPreds = np.array(trainPreds)[:,:,1].T\n",
    "    trainLoss = log_loss(np.ravel(yTrain), np.ravel(trainPreds))\n",
    "    trainLosses.append(trainLoss)\n",
    "    \n",
    "    validPredsIn = clf.predict_proba(xValid[finVars])\n",
    "    validPredsIn = np.array(validPredsIn)[:,:,1].T\n",
    "    validPreds.append(validPredsIn)\n",
    "    valLoss = log_loss(np.ravel(yValid), np.ravel(validPredsIn))\n",
    "    validLosses.append(valLoss)\n",
    "    #get importances\n",
    "    InnerImp = [] \n",
    "    #multioutput importances\n",
    "    for model in clf.estimators_:\n",
    "        InnerImp.append(model.feature_importances_)\n",
    "    featImp.append(np.nanmean(InnerImp, axis=0) )   \n",
    "        \n",
    "validLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.013361012319180098,\n",
       " 0.013277220043316083,\n",
       " 0.013329097282684398,\n",
       " 0.013369980487311903,\n",
       " 0.01329110373426141]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLosses \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predsDf = pd.DataFrame(validPreds[2],columns=yValid.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00103151, 0.00095975, 0.00095558, ..., 0.00125918, 0.0011878 ,\n",
       "         0.00098684],\n",
       "        [0.00093006, 0.00095975, 0.00095558, ..., 0.00104515, 0.00119152,\n",
       "         0.00098684],\n",
       "        [0.00093006, 0.00095975, 0.00095558, ..., 0.00108389, 0.00096257,\n",
       "         0.00098684],\n",
       "        ...,\n",
       "        [0.00093006, 0.00095975, 0.00095558, ..., 0.00108389, 0.00344124,\n",
       "         0.00132718],\n",
       "        [0.00093006, 0.00095975, 0.00095558, ..., 0.00104515, 0.00144277,\n",
       "         0.00098684],\n",
       "        [0.00099724, 0.00095975, 0.00095558, ..., 0.00121418, 0.00160305,\n",
       "         0.00132718]], dtype=float32),\n",
       " array([[0.00115268, 0.0009598 , 0.00096072, ..., 0.00159167, 0.00158951,\n",
       "         0.00130504],\n",
       "        [0.00089855, 0.0009598 , 0.00096072, ..., 0.00136413, 0.00206857,\n",
       "         0.00144281],\n",
       "        [0.00086643, 0.0009598 , 0.00096072, ..., 0.00106278, 0.01253433,\n",
       "         0.00109141],\n",
       "        ...,\n",
       "        [0.00099025, 0.0009598 , 0.00096072, ..., 0.00115178, 0.00169817,\n",
       "         0.00135561],\n",
       "        [0.00086643, 0.0009598 , 0.00096072, ..., 0.00147833, 0.00294291,\n",
       "         0.00128955],\n",
       "        [0.00089855, 0.0009598 , 0.00096072, ..., 0.00124012, 0.00032666,\n",
       "         0.00159702]], dtype=float32),\n",
       " array([[0.00109446, 0.00095975, 0.00096072, ..., 0.00095825, 0.00201491,\n",
       "         0.00117988],\n",
       "        [0.00089823, 0.00095975, 0.00096072, ..., 0.00095825, 0.00411259,\n",
       "         0.00117988],\n",
       "        [0.00093794, 0.00095975, 0.00096072, ..., 0.00095825, 0.00168112,\n",
       "         0.00133551],\n",
       "        ...,\n",
       "        [0.00090184, 0.00095975, 0.00096072, ..., 0.00095825, 0.00063065,\n",
       "         0.00133551],\n",
       "        [0.00101521, 0.00095975, 0.00096072, ..., 0.00095825, 0.00365657,\n",
       "         0.00133551],\n",
       "        [0.00083155, 0.00095975, 0.00096072, ..., 0.00095825, 0.00136143,\n",
       "         0.00133551]], dtype=float32),\n",
       " array([[0.00118649, 0.00095816, 0.00125975, ..., 0.00096072, 0.00129341,\n",
       "         0.00118284],\n",
       "        [0.00118649, 0.00095816, 0.00125975, ..., 0.00096072, 0.00057555,\n",
       "         0.00118284],\n",
       "        [0.00093305, 0.00095816, 0.00125975, ..., 0.00096072, 0.00246192,\n",
       "         0.00114044],\n",
       "        ...,\n",
       "        [0.00086286, 0.00095816, 0.00125975, ..., 0.00096072, 0.00073083,\n",
       "         0.00114044],\n",
       "        [0.00093305, 0.00095816, 0.00125975, ..., 0.00096072, 0.00142434,\n",
       "         0.00128156],\n",
       "        [0.00086286, 0.00095816, 0.00125975, ..., 0.00096072, 0.00416508,\n",
       "         0.00114044]], dtype=float32),\n",
       " array([[0.00095498, 0.00109379, 0.00096072, ..., 0.00092509, 0.00125352,\n",
       "         0.00113855],\n",
       "        [0.00095498, 0.00101598, 0.00096072, ..., 0.00092509, 0.00862874,\n",
       "         0.00114184],\n",
       "        [0.00095498, 0.00105666, 0.00096072, ..., 0.00092509, 0.00167549,\n",
       "         0.00109742],\n",
       "        ...,\n",
       "        [0.00095498, 0.00091   , 0.00096072, ..., 0.00092509, 0.00173304,\n",
       "         0.00109742],\n",
       "        [0.00095498, 0.00105666, 0.00096072, ..., 0.00102997, 0.00496896,\n",
       "         0.00118464],\n",
       "        [0.00095498, 0.00101598, 0.00096072, ..., 0.00092509, 0.00102163,\n",
       "         0.00114184]], dtype=float32)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validPreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.00000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7144.00000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "      <td>7144.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.010617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>0.002771</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.006955</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.001258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006095</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.085755</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.001180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.004351</td>\n",
       "      <td>0.006415</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.001180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.007022</td>\n",
       "      <td>0.011076</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.007920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.001277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.010643</td>\n",
       "      <td>0.017356</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.003630</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.004257</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.064062</td>\n",
       "      <td>0.108557</td>\n",
       "      <td>0.022759</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.026763</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.149884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.005256</td>\n",
       "      <td>0.979829</td>\n",
       "      <td>0.024575</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.803533</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.008051</td>\n",
       "      <td>0.001336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  acat_inhibitor  \\\n",
       "count                  7144.000000              7144.00000     7144.000000   \n",
       "mean                      0.000960                 0.00096        0.000961   \n",
       "std                       0.000135                 0.00000        0.000000   \n",
       "min                       0.000777                 0.00096        0.000961   \n",
       "25%                       0.000839                 0.00096        0.000961   \n",
       "50%                       0.000934                 0.00096        0.000961   \n",
       "75%                       0.001050                 0.00096        0.000961   \n",
       "max                       0.001411                 0.00096        0.000961   \n",
       "\n",
       "       acetylcholine_receptor_agonist  acetylcholine_receptor_antagonist  \\\n",
       "count                     7144.000000                        7144.000000   \n",
       "mean                         0.008281                           0.013029   \n",
       "std                          0.006095                           0.009752   \n",
       "min                          0.000603                           0.000980   \n",
       "25%                          0.004351                           0.006415   \n",
       "50%                          0.007022                           0.011076   \n",
       "75%                          0.010643                           0.017356   \n",
       "max                          0.064062                           0.108557   \n",
       "\n",
       "       acetylcholinesterase_inhibitor  adenosine_receptor_agonist  \\\n",
       "count                     7144.000000                 7144.000000   \n",
       "mean                         0.003252                    0.002082   \n",
       "std                          0.001974                    0.001156   \n",
       "min                          0.000693                    0.000516   \n",
       "25%                          0.001850                    0.001246   \n",
       "50%                          0.002792                    0.001809   \n",
       "75%                          0.004121                    0.002627   \n",
       "max                          0.022759                    0.008983   \n",
       "\n",
       "       adenosine_receptor_antagonist  adenylyl_cyclase_activator  \\\n",
       "count                    7144.000000                 7144.000000   \n",
       "mean                        0.003702                    0.000958   \n",
       "std                         0.002589                    0.000000   \n",
       "min                         0.000531                    0.000958   \n",
       "25%                         0.001968                    0.000958   \n",
       "50%                         0.003077                    0.000958   \n",
       "75%                         0.004663                    0.000958   \n",
       "max                         0.026763                    0.000958   \n",
       "\n",
       "       adrenergic_receptor_agonist  ...  \\\n",
       "count                  7144.000000  ...   \n",
       "mean                      0.010617  ...   \n",
       "std                       0.010552  ...   \n",
       "min                       0.000473  ...   \n",
       "25%                       0.004550  ...   \n",
       "50%                       0.007920  ...   \n",
       "75%                       0.013295  ...   \n",
       "max                       0.149884  ...   \n",
       "\n",
       "       tropomyosin_receptor_kinase_inhibitor  trpv_agonist  trpv_antagonist  \\\n",
       "count                             7144.00000   7144.000000      7144.000000   \n",
       "mean                                 0.00096      0.000960         0.002109   \n",
       "std                                  0.00000      0.000056         0.000593   \n",
       "min                                  0.00096      0.000906         0.000974   \n",
       "25%                                  0.00096      0.000906         0.001660   \n",
       "50%                                  0.00096      0.000906         0.002019   \n",
       "75%                                  0.00096      0.001017         0.002462   \n",
       "max                                  0.00096      0.001017         0.005256   \n",
       "\n",
       "       tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "count        7144.000000                7144.000000   \n",
       "mean            0.013153                   0.002771   \n",
       "std             0.085755                   0.001963   \n",
       "min             0.000407                   0.000427   \n",
       "25%             0.001463                   0.001515   \n",
       "50%             0.002199                   0.002224   \n",
       "75%             0.003630                   0.003386   \n",
       "max             0.979829                   0.024575   \n",
       "\n",
       "       ubiquitin_specific_protease_inhibitor  vegfr_inhibitor    vitamin_b  \\\n",
       "count                            7144.000000      7144.000000  7144.000000   \n",
       "mean                                0.000951         0.006955     0.000958   \n",
       "std                                 0.000000         0.027501     0.000000   \n",
       "min                                 0.000951         0.000813     0.000958   \n",
       "25%                                 0.000951         0.001906     0.000958   \n",
       "50%                                 0.000951         0.002676     0.000958   \n",
       "75%                                 0.000951         0.004257     0.000958   \n",
       "max                                 0.000951         0.803533     0.000958   \n",
       "\n",
       "       vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "count                 7144.000000    7144.000000  \n",
       "mean                     0.001509       0.001258  \n",
       "std                      0.000896       0.000077  \n",
       "min                      0.000554       0.001180  \n",
       "25%                      0.000900       0.001180  \n",
       "50%                      0.001242       0.001277  \n",
       "75%                      0.001786       0.001336  \n",
       "max                      0.008051       0.001336  \n",
       "\n",
       "[8 rows x 206 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsDf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getLogLoss(y,yhat):\n",
    "    '''\n",
    "    logloss\n",
    "    '''\n",
    "    assert len(y)==len(yhat)\n",
    "    eps= 1e-15\n",
    "    \n",
    "    yhat= np.clip(yhat,eps,1-eps)\n",
    "    return -1*np.mean(y*np.log(yhat) + (1-y)*np.log(1-yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDf = pd.DataFrame(yValid.columns,columns=['varname'])\n",
    "resultsDf['logloss'] = resultsDf['varname'].map(lambda var: getLogLoss(yValid[var].values,predsDf[var].values) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDf.sort_values(by=['logloss'],ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDf['valCount'] = resultsDf['varname'].map(lambda var: yValid[var].sum()) \n",
    "resultsDf['Prop'] = resultsDf['varname'].map(lambda var: yValid[var].sum()/(yValid.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>varname</th>\n",
       "      <th>logloss</th>\n",
       "      <th>valCount</th>\n",
       "      <th>Prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>proteasome_inhibitor</td>\n",
       "      <td>0.347513</td>\n",
       "      <td>213</td>\n",
       "      <td>0.029815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>nfkb_inhibitor</td>\n",
       "      <td>0.305695</td>\n",
       "      <td>253</td>\n",
       "      <td>0.035414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>tubulin_inhibitor</td>\n",
       "      <td>0.112584</td>\n",
       "      <td>105</td>\n",
       "      <td>0.014698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>cdk_inhibitor</td>\n",
       "      <td>0.111435</td>\n",
       "      <td>96</td>\n",
       "      <td>0.013438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>dna_inhibitor</td>\n",
       "      <td>0.103509</td>\n",
       "      <td>142</td>\n",
       "      <td>0.019877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>glucocorticoid_receptor_agonist</td>\n",
       "      <td>0.100165</td>\n",
       "      <td>87</td>\n",
       "      <td>0.012178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>egfr_inhibitor</td>\n",
       "      <td>0.095581</td>\n",
       "      <td>95</td>\n",
       "      <td>0.013298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>cyclooxygenase_inhibitor</td>\n",
       "      <td>0.095554</td>\n",
       "      <td>129</td>\n",
       "      <td>0.018057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>dopamine_receptor_antagonist</td>\n",
       "      <td>0.093576</td>\n",
       "      <td>124</td>\n",
       "      <td>0.017357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>serotonin_receptor_antagonist</td>\n",
       "      <td>0.092815</td>\n",
       "      <td>124</td>\n",
       "      <td>0.017357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>hmgcr_inhibitor</td>\n",
       "      <td>0.091601</td>\n",
       "      <td>85</td>\n",
       "      <td>0.011898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>pdgfr_inhibitor</td>\n",
       "      <td>0.089118</td>\n",
       "      <td>82</td>\n",
       "      <td>0.011478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>raf_inhibitor</td>\n",
       "      <td>0.086265</td>\n",
       "      <td>68</td>\n",
       "      <td>0.009518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>kit_inhibitor</td>\n",
       "      <td>0.083349</td>\n",
       "      <td>75</td>\n",
       "      <td>0.010498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>glutamate_receptor_antagonist</td>\n",
       "      <td>0.078951</td>\n",
       "      <td>101</td>\n",
       "      <td>0.014138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acetylcholine_receptor_antagonist</td>\n",
       "      <td>0.077813</td>\n",
       "      <td>97</td>\n",
       "      <td>0.013578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adrenergic_receptor_antagonist</td>\n",
       "      <td>0.073515</td>\n",
       "      <td>92</td>\n",
       "      <td>0.012878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>flt3_inhibitor</td>\n",
       "      <td>0.072017</td>\n",
       "      <td>70</td>\n",
       "      <td>0.009798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>sodium_channel_inhibitor</td>\n",
       "      <td>0.068636</td>\n",
       "      <td>87</td>\n",
       "      <td>0.012178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adrenergic_receptor_agonist</td>\n",
       "      <td>0.066829</td>\n",
       "      <td>80</td>\n",
       "      <td>0.011198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>serotonin_receptor_agonist</td>\n",
       "      <td>0.061471</td>\n",
       "      <td>76</td>\n",
       "      <td>0.010638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>calcium_channel_blocker</td>\n",
       "      <td>0.059665</td>\n",
       "      <td>73</td>\n",
       "      <td>0.010218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>histamine_receptor_antagonist</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>69</td>\n",
       "      <td>0.009658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>phosphodiesterase_inhibitor</td>\n",
       "      <td>0.056603</td>\n",
       "      <td>69</td>\n",
       "      <td>0.009658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>vegfr_inhibitor</td>\n",
       "      <td>0.055195</td>\n",
       "      <td>60</td>\n",
       "      <td>0.008399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acetylcholine_receptor_agonist</td>\n",
       "      <td>0.050164</td>\n",
       "      <td>60</td>\n",
       "      <td>0.008399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>topoisomerase_inhibitor</td>\n",
       "      <td>0.047258</td>\n",
       "      <td>44</td>\n",
       "      <td>0.006159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>bacterial_cell_wall_synthesis_inhibitor</td>\n",
       "      <td>0.045211</td>\n",
       "      <td>54</td>\n",
       "      <td>0.007559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>estrogen_receptor_agonist</td>\n",
       "      <td>0.040326</td>\n",
       "      <td>46</td>\n",
       "      <td>0.006439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>mtor_inhibitor</td>\n",
       "      <td>0.039267</td>\n",
       "      <td>36</td>\n",
       "      <td>0.005039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>pi3k_inhibitor</td>\n",
       "      <td>0.039261</td>\n",
       "      <td>42</td>\n",
       "      <td>0.005879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>gaba_receptor_antagonist</td>\n",
       "      <td>0.037616</td>\n",
       "      <td>42</td>\n",
       "      <td>0.005879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>hsp_inhibitor</td>\n",
       "      <td>0.035438</td>\n",
       "      <td>31</td>\n",
       "      <td>0.004339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>potassium_channel_antagonist</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>38</td>\n",
       "      <td>0.005319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>progesterone_receptor_agonist</td>\n",
       "      <td>0.034071</td>\n",
       "      <td>36</td>\n",
       "      <td>0.005039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>dopamine_receptor_agonist</td>\n",
       "      <td>0.033730</td>\n",
       "      <td>37</td>\n",
       "      <td>0.005179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>bacterial_dna_inhibitor</td>\n",
       "      <td>0.033636</td>\n",
       "      <td>37</td>\n",
       "      <td>0.005179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>cytochrome_p450_inhibitor</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>38</td>\n",
       "      <td>0.005319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>gaba_receptor_agonist</td>\n",
       "      <td>0.032740</td>\n",
       "      <td>35</td>\n",
       "      <td>0.004899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>hdac_inhibitor</td>\n",
       "      <td>0.030955</td>\n",
       "      <td>24</td>\n",
       "      <td>0.003359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>androgen_receptor_antagonist</td>\n",
       "      <td>0.030805</td>\n",
       "      <td>33</td>\n",
       "      <td>0.004619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>opioid_receptor_antagonist</td>\n",
       "      <td>0.030429</td>\n",
       "      <td>34</td>\n",
       "      <td>0.004759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>aurora_kinase_inhibitor</td>\n",
       "      <td>0.029551</td>\n",
       "      <td>28</td>\n",
       "      <td>0.003919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>atpase_inhibitor</td>\n",
       "      <td>0.029490</td>\n",
       "      <td>30</td>\n",
       "      <td>0.004199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>ppar_receptor_agonist</td>\n",
       "      <td>0.028391</td>\n",
       "      <td>31</td>\n",
       "      <td>0.004339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>anesthetic_-_local</td>\n",
       "      <td>0.027286</td>\n",
       "      <td>29</td>\n",
       "      <td>0.004059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>prostanoid_receptor_antagonist</td>\n",
       "      <td>0.026879</td>\n",
       "      <td>29</td>\n",
       "      <td>0.004059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>mek_inhibitor</td>\n",
       "      <td>0.025750</td>\n",
       "      <td>22</td>\n",
       "      <td>0.003080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>bacterial_50s_ribosomal_subunit_inhibitor</td>\n",
       "      <td>0.025401</td>\n",
       "      <td>26</td>\n",
       "      <td>0.003639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>src_inhibitor</td>\n",
       "      <td>0.025087</td>\n",
       "      <td>24</td>\n",
       "      <td>0.003359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       varname   logloss  valCount      Prop\n",
       "163                       proteasome_inhibitor  0.347513       213  0.029815\n",
       "136                             nfkb_inhibitor  0.305695       253  0.035414\n",
       "199                          tubulin_inhibitor  0.112584       105  0.014698\n",
       "63                               cdk_inhibitor  0.111435        96  0.013438\n",
       "77                               dna_inhibitor  0.103509       142  0.019877\n",
       "96             glucocorticoid_receptor_agonist  0.100165        87  0.012178\n",
       "80                              egfr_inhibitor  0.095581        95  0.013298\n",
       "71                    cyclooxygenase_inhibitor  0.095554       129  0.018057\n",
       "79                dopamine_receptor_antagonist  0.093576       124  0.017357\n",
       "177              serotonin_receptor_antagonist  0.092815       124  0.017357\n",
       "109                            hmgcr_inhibitor  0.091601        85  0.011898\n",
       "149                            pdgfr_inhibitor  0.089118        82  0.011478\n",
       "169                              raf_inhibitor  0.086265        68  0.009518\n",
       "119                              kit_inhibitor  0.083349        75  0.010498\n",
       "99               glutamate_receptor_antagonist  0.078951       101  0.014138\n",
       "4            acetylcholine_receptor_antagonist  0.077813        97  0.013578\n",
       "10              adrenergic_receptor_antagonist  0.073515        92  0.012878\n",
       "89                              flt3_inhibitor  0.072017        70  0.009798\n",
       "182                   sodium_channel_inhibitor  0.068636        87  0.012178\n",
       "9                  adrenergic_receptor_agonist  0.066829        80  0.011198\n",
       "176                 serotonin_receptor_agonist  0.061471        76  0.010638\n",
       "54                     calcium_channel_blocker  0.059665        73  0.010218\n",
       "105              histamine_receptor_antagonist  0.056780        69  0.009658\n",
       "151                phosphodiesterase_inhibitor  0.056603        69  0.009658\n",
       "202                            vegfr_inhibitor  0.055195        60  0.008399\n",
       "3               acetylcholine_receptor_agonist  0.050164        60  0.008399\n",
       "194                    topoisomerase_inhibitor  0.047258        44  0.006159\n",
       "43     bacterial_cell_wall_synthesis_inhibitor  0.045211        54  0.007559\n",
       "83                   estrogen_receptor_agonist  0.040326        46  0.006439\n",
       "133                             mtor_inhibitor  0.039267        36  0.005039\n",
       "153                             pi3k_inhibitor  0.039261        42  0.005879\n",
       "94                    gaba_receptor_antagonist  0.037616        42  0.005879\n",
       "110                              hsp_inhibitor  0.035438        31  0.004339\n",
       "156               potassium_channel_antagonist  0.034100        38  0.005319\n",
       "159              progesterone_receptor_agonist  0.034071        36  0.005039\n",
       "78                   dopamine_receptor_agonist  0.033730        37  0.005179\n",
       "45                     bacterial_dna_inhibitor  0.033636        37  0.005179\n",
       "72                   cytochrome_p450_inhibitor  0.033613        38  0.005319\n",
       "93                       gaba_receptor_agonist  0.032740        35  0.004899\n",
       "103                             hdac_inhibitor  0.030955        24  0.003359\n",
       "17                androgen_receptor_antagonist  0.030805        33  0.004619\n",
       "144                 opioid_receptor_antagonist  0.030429        34  0.004759\n",
       "38                     aurora_kinase_inhibitor  0.029551        28  0.003919\n",
       "36                            atpase_inhibitor  0.029490        30  0.004199\n",
       "157                      ppar_receptor_agonist  0.028391        31  0.004339\n",
       "18                          anesthetic_-_local  0.027286        29  0.004059\n",
       "162             prostanoid_receptor_antagonist  0.026879        29  0.004059\n",
       "127                              mek_inhibitor  0.025750        22  0.003080\n",
       "41   bacterial_50s_ribosomal_subunit_inhibitor  0.025401        26  0.003639\n",
       "184                              src_inhibitor  0.025087        24  0.003359"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsDf.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024560134722327678"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsDf['logloss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>varname</th>\n",
       "      <th>logloss</th>\n",
       "      <th>valCount</th>\n",
       "      <th>Prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>histone_lysine_demethylase_inhibitor</td>\n",
       "      <td>0.006085</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>fungal_squalene_epoxidase_inhibitor</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>glutamate_inhibitor</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>fatty_acid_receptor_agonist</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11-beta-hsd1_inhibitor</td>\n",
       "      <td>0.005662</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>imidazoline_receptor_agonist</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>nitric_oxide_synthase_inhibitor</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>atr_kinase_inhibitor</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>beta_amyloid_inhibitor</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>nitric_oxide_donor</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>cck_receptor_antagonist</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>free_radical_scavenger</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>lipase_inhibitor</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>monoacylglycerol_lipase_inhibitor</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ampk_activator</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>ras_gtpase_inhibitor</td>\n",
       "      <td>0.004722</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5-alpha_reductase_inhibitor</td>\n",
       "      <td>0.004609</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>sphingosine_receptor_agonist</td>\n",
       "      <td>0.004043</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>monopolar_spindle_1_kinase_inhibitor</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>protein_phosphatase_inhibitor</td>\n",
       "      <td>0.003801</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>elastase_inhibitor</td>\n",
       "      <td>0.003801</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>laxative</td>\n",
       "      <td>0.003801</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>norepinephrine_reuptake_inhibitor</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>antihistamine</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>atp_synthase_inhibitor</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>nrf2_activator</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>proteasome_inhibitor</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>222</td>\n",
       "      <td>0.031075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>farnesyltransferase_inhibitor</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>ubiquitin_specific_protease_inhibitor</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>tropomyosin_receptor_kinase_inhibitor</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>leukotriene_inhibitor</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>diuretic</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>lxr_agonist</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>atm_kinase_inhibitor</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>tlr_antagonist</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aldehyde_dehydrogenase_inhibitor</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>nitric_oxide_production_inhibitor</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>tgf-beta_receptor_inhibitor</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>analgesic</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>retinoid_receptor_antagonist</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>steroid</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>antiarrhythmic</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>bacterial_membrane_integrity_inhibitor</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>anticonvulsant</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>coagulation_factor_inhibitor</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>nicotinic_receptor_agonist</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>calcineurin_inhibitor</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>autotaxin_inhibitor</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>erbb2_inhibitor</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>atp-sensitive_potassium_channel_antagonist</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        varname   logloss  valCount      Prop\n",
       "106        histone_lysine_demethylase_inhibitor  0.006085         6  0.000840\n",
       "92          fungal_squalene_epoxidase_inhibitor  0.005911         5  0.000700\n",
       "97                          glutamate_inhibitor  0.005859         5  0.000700\n",
       "87                  fatty_acid_receptor_agonist  0.005749         5  0.000700\n",
       "1                        11-beta-hsd1_inhibitor  0.005662         5  0.000700\n",
       "113                imidazoline_receptor_agonist  0.005342         4  0.000560\n",
       "140             nitric_oxide_synthase_inhibitor  0.005115         4  0.000560\n",
       "37                         atr_kinase_inhibitor  0.005048         5  0.000700\n",
       "50                       beta_amyloid_inhibitor  0.005023         4  0.000560\n",
       "138                          nitric_oxide_donor  0.004947         4  0.000560\n",
       "62                      cck_receptor_antagonist  0.004810         4  0.000560\n",
       "91                       free_radical_scavenger  0.004805         4  0.000560\n",
       "123                            lipase_inhibitor  0.004793         4  0.000560\n",
       "130           monoacylglycerol_lipase_inhibitor  0.004741         4  0.000560\n",
       "14                               ampk_activator  0.004730         4  0.000560\n",
       "170                        ras_gtpase_inhibitor  0.004722         4  0.000560\n",
       "0                   5-alpha_reductase_inhibitor  0.004609         4  0.000560\n",
       "183                sphingosine_receptor_agonist  0.004043         3  0.000420\n",
       "132        monopolar_spindle_1_kinase_inhibitor  0.003818         3  0.000420\n",
       "165               protein_phosphatase_inhibitor  0.003801         3  0.000420\n",
       "81                           elastase_inhibitor  0.003801         3  0.000420\n",
       "120                                    laxative  0.003801         3  0.000420\n",
       "141           norepinephrine_reuptake_inhibitor  0.003740         3  0.000420\n",
       "26                                antihistamine  0.003650         3  0.000420\n",
       "35                       atp_synthase_inhibitor  0.003104         3  0.000420\n",
       "142                              nrf2_activator  0.003001         3  0.000420\n",
       "163                        proteasome_inhibitor  0.002682       222  0.031075\n",
       "86                farnesyltransferase_inhibitor  0.002646         2  0.000280\n",
       "201       ubiquitin_specific_protease_inhibitor  0.002573         2  0.000280\n",
       "196       tropomyosin_receptor_kinase_inhibitor  0.002573         2  0.000280\n",
       "121                       leukotriene_inhibitor  0.002573         2  0.000280\n",
       "75                                     diuretic  0.002573         2  0.000280\n",
       "125                                 lxr_agonist  0.002573         2  0.000280\n",
       "33                         atm_kinase_inhibitor  0.002573         2  0.000280\n",
       "192                              tlr_antagonist  0.002571         2  0.000280\n",
       "12             aldehyde_dehydrogenase_inhibitor  0.002551         2  0.000280\n",
       "139           nitric_oxide_production_inhibitor  0.002493         2  0.000280\n",
       "188                 tgf-beta_receptor_inhibitor  0.001697         3  0.000420\n",
       "15                                    analgesic  0.001654         1  0.000140\n",
       "172                retinoid_receptor_antagonist  0.001435         1  0.000140\n",
       "185                                     steroid  0.001435         1  0.000140\n",
       "22                               antiarrhythmic  0.001435         1  0.000140\n",
       "46       bacterial_membrane_integrity_inhibitor  0.001357         1  0.000140\n",
       "24                               anticonvulsant  0.000722         0  0.000000\n",
       "69                 coagulation_factor_inhibitor  0.000360         0  0.000000\n",
       "137                  nicotinic_receptor_agonist  0.000360         0  0.000000\n",
       "53                        calcineurin_inhibitor  0.000360         0  0.000000\n",
       "39                          autotaxin_inhibitor  0.000360         0  0.000000\n",
       "82                              erbb2_inhibitor  0.000060         0  0.000000\n",
       "34   atp-sensitive_potassium_channel_antagonist  0.000060         0  0.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsDf.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7144.000000\n",
       "mean        0.004247\n",
       "std         0.007690\n",
       "min         0.000010\n",
       "25%         0.000827\n",
       "50%         0.002025\n",
       "75%         0.004691\n",
       "max         0.160498\n",
       "Name: cyclooxygenase_inhibitor, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsDf['cyclooxygenase_inhibitor'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7144.000000\n",
       "mean        0.019317\n",
       "std         0.137646\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000\n",
       "Name: cyclooxygenase_inhibitor, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yValid['cyclooxygenase_inhibitor'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsTemp = pd.DataFrame(yValid['cyclooxygenase_inhibitor']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsTemp['Predictions'] =  predsDf['cyclooxygenase_inhibitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cyclooxygenase_inhibitor</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.006694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.015550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cyclooxygenase_inhibitor  Predictions\n",
       "0                         0     0.006694\n",
       "1                         0     0.002841\n",
       "2                         0     0.015550\n",
       "3                         0     0.001458\n",
       "4                         0     0.001156"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsTemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cyclooxygenase_inhibitor</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>1</td>\n",
       "      <td>0.160498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316</th>\n",
       "      <td>0</td>\n",
       "      <td>0.128508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>0</td>\n",
       "      <td>0.113263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7019</th>\n",
       "      <td>0</td>\n",
       "      <td>0.108153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>0</td>\n",
       "      <td>0.107008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7144 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cyclooxygenase_inhibitor  Predictions\n",
       "6996                         1     0.160498\n",
       "6316                         0     0.128508\n",
       "1333                         0     0.113263\n",
       "7019                         0     0.108153\n",
       "2453                         0     0.107008\n",
       "...                        ...          ...\n",
       "3238                         0     0.000018\n",
       "4029                         0     0.000017\n",
       "1929                         0     0.000016\n",
       "6493                         0     0.000014\n",
       "5882                         0     0.000010\n",
       "\n",
       "[7144 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsTemp.sort_values(by=['Predictions'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cyclooxygenase_inhibitor</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>zeros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6545</th>\n",
       "      <td>1</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3027</th>\n",
       "      <td>1</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>1</td>\n",
       "      <td>0.160498</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6335</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>1</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3454</th>\n",
       "      <td>1</td>\n",
       "      <td>0.016020</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>1</td>\n",
       "      <td>0.006857</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>1</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3762</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>1</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3030</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>1</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>1</td>\n",
       "      <td>0.008268</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>1</td>\n",
       "      <td>0.014738</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6073</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002655</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>1</td>\n",
       "      <td>0.016321</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cyclooxygenase_inhibitor  Predictions  zeros\n",
       "2297                         1     0.001864   0.04\n",
       "6545                         1     0.006366   0.04\n",
       "3027                         1     0.003437   0.04\n",
       "6996                         1     0.160498   0.04\n",
       "6335                         1     0.000741   0.04\n",
       "1459                         1     0.001472   0.04\n",
       "4386                         1     0.012031   0.04\n",
       "3454                         1     0.016020   0.04\n",
       "6090                         1     0.006857   0.04\n",
       "3451                         1     0.004214   0.04\n",
       "3762                         1     0.001727   0.04\n",
       "4378                         1     0.005895   0.04\n",
       "3030                         1     0.001540   0.04\n",
       "5494                         1     0.002069   0.04\n",
       "806                          1     0.007036   0.04\n",
       "1085                         1     0.008268   0.04\n",
       "5915                         1     0.014738   0.04\n",
       "6073                         1     0.002655   0.04\n",
       "377                          1     0.016321   0.04\n",
       "5178                         1     0.001557   0.04"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsTemp.sort_values(by=['cyclooxygenase_inhibitor'],ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7144.000000\n",
       "mean        0.004247\n",
       "std         0.007690\n",
       "min         0.000010\n",
       "25%         0.000827\n",
       "50%         0.002025\n",
       "75%         0.004691\n",
       "max         0.160498\n",
       "Name: cyclooxygenase_inhibitor, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsDf['cyclooxygenase_inhibitor'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10715562620476837"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLogLoss(predsTemp['cyclooxygenase_inhibitor'],predsTemp['Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10221217209416565"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsTemp['zeros']= 0.04\n",
    "\n",
    "getLogLoss(predsTemp['cyclooxygenase_inhibitor'],predsTemp['zeros'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13441765591218102"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLogLoss(predsTemp['cyclooxygenase_inhibitor'],predsTemp['zeros'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10715562620476837"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLogLoss(predsTemp['cyclooxygenase_inhibitor'],predsTemp['Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPreds = validPreds[0]/5\n",
    "for i in range(1,5):\n",
    "    allPreds+=validPreds[i]/5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018709705955952918"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(np.ravel(yValid), np.ravel(allPreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lish-moa.zip\t       test_features.csv   train_targets_nonscored.csv\r\n",
      "sample_submission.csv  train_features.csv  train_targets_scored.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest = pd.read_csv('../Data/test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest = PreProcessX(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredsIn = clf.predict_proba(xTest[finVars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3982, 2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPredsIn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999045"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPredsIn[0][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3982, 206)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(testPredsIn)[:,:,1].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[yValid.columns] = np.array(testPredsIn)[:,:,1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.010790</td>\n",
       "      <td>0.013946</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.004011</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.001139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.005954</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.001139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>0.004309</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>0.001185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.010180</td>\n",
       "      <td>0.005725</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.012388</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>0.010924</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.001185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.010439</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.001097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                     0.000955                0.000845   \n",
       "1  id_001897cda                     0.000955                0.000845   \n",
       "2  id_002429b5b                     0.000955                0.000845   \n",
       "3  id_00276f245                     0.000955                0.001016   \n",
       "4  id_0027f1083                     0.000955                0.000845   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0        0.000961                        0.010790   \n",
       "1        0.000961                        0.001523   \n",
       "2        0.000961                        0.010769   \n",
       "3        0.000961                        0.002953   \n",
       "4        0.000961                        0.010439   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                           0.013946                        0.005172   \n",
       "1                           0.005387                        0.004012   \n",
       "2                           0.004309                        0.002344   \n",
       "3                           0.010180                        0.005725   \n",
       "4                           0.005950                        0.001218   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                    0.004011                       0.006444   \n",
       "1                    0.001121                       0.002132   \n",
       "2                    0.001036                       0.004060   \n",
       "3                    0.001361                       0.012388   \n",
       "4                    0.002743                       0.002373   \n",
       "\n",
       "   adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                    0.000958  ...                               0.000936   \n",
       "1                    0.000958  ...                               0.000936   \n",
       "2                    0.000958  ...                               0.000936   \n",
       "3                    0.000958  ...                               0.000936   \n",
       "4                    0.000958  ...                               0.000936   \n",
       "\n",
       "   trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0      0.001072         0.001932           0.001563   \n",
       "1      0.001195         0.001554           0.001973   \n",
       "2      0.001072         0.002231           0.003542   \n",
       "3      0.001171         0.002597           0.010924   \n",
       "4      0.001072         0.003120           0.001946   \n",
       "\n",
       "   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                   0.002130                               0.000936   \n",
       "1                   0.006544                               0.000936   \n",
       "2                   0.000970                               0.000936   \n",
       "3                   0.002643                               0.000936   \n",
       "4                   0.000821                               0.000936   \n",
       "\n",
       "   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0         0.002820   0.001030                    0.002520       0.001139  \n",
       "1         0.005954   0.000925                    0.002713       0.001139  \n",
       "2         0.001411   0.000925                    0.004128       0.001185  \n",
       "3         0.001882   0.001030                    0.003467       0.001185  \n",
       "4         0.001116   0.001030                    0.000506       0.001097  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g-484',\n",
       " 'g-202',\n",
       " 'g-260',\n",
       " 'g-471',\n",
       " 'g-364',\n",
       " 'c-45',\n",
       " 'c-86',\n",
       " 'c-40',\n",
       " 'g-717',\n",
       " 'g-376',\n",
       " 'g-565',\n",
       " 'g-3',\n",
       " 'g-662',\n",
       " 'g-692',\n",
       " 'g-288',\n",
       " 'g-500',\n",
       " 'g-673',\n",
       " 'g-280',\n",
       " 'g-261',\n",
       " 'g-285',\n",
       " 'g-243',\n",
       " 'g-40',\n",
       " 'g-10',\n",
       " 'g-746',\n",
       " 'g-59',\n",
       " 'g-264',\n",
       " 'g-36',\n",
       " 'c-58',\n",
       " 'g-144',\n",
       " 'g-758',\n",
       " 'g-516',\n",
       " 'g-132',\n",
       " 'g-400',\n",
       " 'g-485',\n",
       " 'g-512',\n",
       " 'g-61',\n",
       " 'g-498',\n",
       " 'c-61',\n",
       " 'g-416',\n",
       " 'c-20',\n",
       " 'g-759',\n",
       " 'g-91',\n",
       " 'g-604',\n",
       " 'g-463',\n",
       " 'cp_type',\n",
       " 'g-689',\n",
       " 'g-134',\n",
       " 'g-408',\n",
       " 'g-587',\n",
       " 'g-14',\n",
       " 'g-158',\n",
       " 'g-140',\n",
       " 'g-574',\n",
       " 'g-162',\n",
       " 'g-386',\n",
       " 'g-43',\n",
       " 'g-201',\n",
       " 'c-68',\n",
       " 'g-702',\n",
       " 'g-539',\n",
       " 'g-173',\n",
       " 'g-653',\n",
       " 'c-90',\n",
       " 'g-578',\n",
       " 'g-22',\n",
       " 'g-392',\n",
       " 'g-541',\n",
       " 'c-63',\n",
       " 'g-296',\n",
       " 'g-630',\n",
       " 'g-339',\n",
       " 'g-234',\n",
       " 'g-627',\n",
       " 'g-203',\n",
       " 'g-310',\n",
       " 'g-85',\n",
       " 'g-700',\n",
       " 'g-343',\n",
       " 'g-38',\n",
       " 'c-0',\n",
       " 'c-35',\n",
       " 'c-92',\n",
       " 'c-89',\n",
       " 'g-136',\n",
       " 'g-79',\n",
       " 'c-7',\n",
       " 'g-210',\n",
       " 'g-750',\n",
       " 'c-24',\n",
       " 'g-94',\n",
       " 'g-699',\n",
       " 'g-327',\n",
       " 'g-474',\n",
       " 'g-491',\n",
       " 'g-613',\n",
       " 'g-575',\n",
       " 'g-567',\n",
       " 'g-577',\n",
       " 'c-93',\n",
       " 'g-369',\n",
       " 'c-85',\n",
       " 'g-90',\n",
       " 'g-486',\n",
       " 'g-643',\n",
       " 'g-127',\n",
       " 'g-566',\n",
       " 'g-765',\n",
       " 'g-237',\n",
       " 'g-470',\n",
       " 'g-413',\n",
       " 'g-552',\n",
       " 'g-526',\n",
       " 'g-23',\n",
       " 'g-169',\n",
       " 'g-2',\n",
       " 'g-629',\n",
       " 'g-679',\n",
       " 'g-60',\n",
       " 'c-28',\n",
       " 'g-396',\n",
       " 'g-186',\n",
       " 'g-101',\n",
       " 'g-198',\n",
       " 'g-464',\n",
       " 'g-589',\n",
       " 'g-196',\n",
       " 'c-65',\n",
       " 'g-56',\n",
       " 'g-345',\n",
       " 'g-462',\n",
       " 'g-305',\n",
       " 'g-314',\n",
       " 'g-361',\n",
       " 'c-64',\n",
       " 'g-209',\n",
       " 'g-313',\n",
       " 'g-737',\n",
       " 'g-609',\n",
       " 'g-8',\n",
       " 'c-1',\n",
       " 'g-727',\n",
       " 'g-622',\n",
       " 'c-23',\n",
       " 'g-97',\n",
       " 'g-564',\n",
       " 'g-466',\n",
       " 'g-525',\n",
       " 'g-767',\n",
       " 'g-250',\n",
       " 'g-297',\n",
       " 'g-725',\n",
       " 'g-214',\n",
       " 'g-414',\n",
       " 'g-105',\n",
       " 'g-383',\n",
       " 'g-655',\n",
       " 'g-17',\n",
       " 'g-256',\n",
       " 'g-418',\n",
       " 'g-115',\n",
       " 'g-453',\n",
       " 'g-705',\n",
       " 'g-559',\n",
       " 'g-228',\n",
       " 'g-212',\n",
       " 'g-571',\n",
       " 'g-247',\n",
       " 'g-199',\n",
       " 'g-352',\n",
       " 'g-573',\n",
       " 'c-56',\n",
       " 'g-142',\n",
       " 'g-624',\n",
       " 'g-100',\n",
       " 'g-289',\n",
       " 'g-547',\n",
       " 'g-381',\n",
       " 'c-98',\n",
       " 'g-742',\n",
       " 'g-235',\n",
       " 'g-337',\n",
       " 'g-157',\n",
       " 'g-531',\n",
       " 'g-753',\n",
       " 'g-419',\n",
       " 'g-683',\n",
       " 'g-685',\n",
       " 'c-39',\n",
       " 'g-676',\n",
       " 'g-748',\n",
       " 'g-563',\n",
       " 'g-659',\n",
       " 'g-657',\n",
       " 'g-278',\n",
       " 'g-703',\n",
       " 'g-138',\n",
       " 'g-405',\n",
       " 'g-346',\n",
       " 'g-322',\n",
       " 'g-206',\n",
       " 'g-660',\n",
       " 'g-515',\n",
       " 'g-650',\n",
       " 'g-87',\n",
       " 'g-557',\n",
       " 'g-709',\n",
       " 'g-634',\n",
       " 'g-0',\n",
       " 'g-599',\n",
       " 'g-656',\n",
       " 'g-735',\n",
       " 'g-320',\n",
       " 'g-309',\n",
       " 'g-439',\n",
       " 'g-244',\n",
       " 'g-511',\n",
       " 'g-544',\n",
       " 'g-693',\n",
       " 'g-180']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finVars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
